{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d6f38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "from sglm.models import sglm_cv\n",
    "import itertools\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sglm.features import gen_signal_df as gsd\n",
    "from sglm.features import build_features as bf\n",
    "from sglm.features import setup_model_fit as smf\n",
    "from sglm.models import sglm_cv\n",
    "from sglm import models\n",
    "from sglm.visualization import visualize\n",
    "from sglm.models import train_model\n",
    "from sglm.models import eval\n",
    "from sglm import features\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import traceback\n",
    "import warnings\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3288d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From stack overflow\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2321b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_multifiles(wt_used, signal_files, widest_orders):\n",
    "    # needed info\n",
    "    # Params: wt_used, signal_files, widest_orders, multifile_fit\n",
    "    # Returns: mouse_names, combo_dfs, combo_fns, X_cols_sftd\n",
    "\n",
    "    file_ids = [Path(_).parts[-1] for _ in signal_files]\n",
    "    print('file_ids', file_ids)\n",
    "    combo_dfs, X_cols_sftd, _ = smf.multi_file_analysis_prep(signal_files, widest_orders, file_ids)\n",
    "    combo_fns = ['_'.join(wt_used).replace('WT', '').replace('S', '')]\n",
    "    mouse_names = combo_fns\n",
    "\n",
    "    return mouse_names, combo_dfs, combo_fns, X_cols_sftd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1924b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_dict_to_json(source_dict, json_path):\n",
    "    with open(str((json_path).resolve()), 'w') as json_file:\n",
    "        json.dump(source_dict, json_file)\n",
    "\n",
    "def read_json(json_path):\n",
    "    with open(str((json_path).resolve()), 'r') as json_file:\n",
    "        read_json = json.load(json_file)\n",
    "    return read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e3485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_rows_with_all_cols(dfrel_basis,\n",
    "                          X_y_pairings,\n",
    "                          X_cols_sftd, \n",
    "                          drop_cols_basis = ['nTrial', 'nTrial_filenum', 'cpn', 'cpx', 'spnnr', 'spxnr', 'spnr', 'spxr',\n",
    "                                             \n",
    "                                             'photometryCenterInIndex', 'photometryCenterOutIndex',\n",
    "                                             'photometrySideInIndexr', 'photometrySideInIndexnr',\n",
    "                                             'photometrySideOutIndex', 'spnnrOff',\n",
    "                                             \n",
    "                                             'photometrySideInIndexAA', 'photometrySideInIndexAa',\n",
    "                                             'photometrySideInIndexaA', 'photometrySideInIndexaa',\n",
    "                                             'photometrySideInIndexAB', 'photometrySideInIndexAb',\n",
    "                                             'photometrySideInIndexaB', 'photometrySideInIndexab',\n",
    "                                             'sl', 'slOff'\n",
    "                                            ]):\n",
    "    full_drop_basis = []\n",
    "    y_col_lst = []\n",
    "    for X_y_dct in X_y_pairings:\n",
    "        full_drop_basis += bf.col_shift_bounds_dict_to_col_list(X_y_dct['X_cols'], X_cols_sftd)\n",
    "        y_col_lst += [X_y_dct['y_col']]\n",
    "    y_col_drop_basis = sorted(list(set(y_col_lst)))\n",
    "    full_drop_basis = sorted(list(set(drop_cols_basis + full_drop_basis + y_col_drop_basis)))\n",
    "\n",
    "    num_cols_na = (dfrel_basis[full_drop_basis].isna().sum(axis=1))\n",
    "    num_y_0 = (dfrel_basis[y_col_drop_basis] == 0).sum(axis=1)\n",
    "    has_all_cols = (num_cols_na == 0)&(num_y_0 == 0)\n",
    "\n",
    "    return has_all_cols\n",
    "\n",
    "def warn_with_traceback(message, category, filename, lineno, file=None, line=None):\n",
    "\n",
    "    log = file if hasattr(file,'write') else sys.stderr\n",
    "    traceback.print_stack(file=log)\n",
    "    log.write(warnings.formatwarning(message, category, filename, lineno, line))\n",
    "\n",
    "warnings.showwarning = warn_with_traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "077d7429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_holdout_and_cv_idx(df_available, pholdout, folds, pgss, filter_series, id_cols=['nTrial_filenum']):\n",
    "    holdout_series = models.split_data.holdout_split_by_trial_id(df_available,\n",
    "                                                                id_cols=id_cols,\n",
    "                                                                perc_holdout=pholdout)\n",
    "    cv_idx_lst = models.split_data.cv_idx_by_trial_id(dfrel_basis_has_all_cols[(~holdout_series)&(filter_series)],\n",
    "                                                             trial_id_columns=id_cols,\n",
    "                                                             num_folds=folds,\n",
    "                                                             test_size=pgss)\n",
    "    return holdout_series, cv_idx_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284536d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d303b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b090735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2622718a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99094c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "714187fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 1\n",
    "param_set_json_name = 'full_model_param_set.json'\n",
    "\n",
    "source_data_path = Path(r'C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\interim')\n",
    "output_path = Path(r'C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\outputs_clean')\n",
    "preproc_params_name = r'preproc_params.json'\n",
    "basis_name = r'_basis.csv'\n",
    "figname = 'Figure_6'\n",
    "groupid = 'g1'\n",
    "param_set_id = '-20_+20'\n",
    "neg_order = -20\n",
    "pos_order = 20\n",
    "fix_training = True\n",
    "num_runs = 10\n",
    "folds = 10\n",
    "pholdout = 0.5\n",
    "pgss = 0.2\n",
    "score_method = 'r2'\n",
    "val_test_inx_sel_method = 'irun'\n",
    "\n",
    "X_y_pairings_lst = []\n",
    "X_y_pairings_lst += [[\n",
    "    {'X_cols': {\n",
    "                'photometryCenterInIndex':(0,0),\n",
    "#                 'photometryCenterOutIndex':(0,0),\n",
    "                'photometrySideInIndex':(0,0),\n",
    "                'photometrySideInIndexr':(0,0),                \n",
    "                'photometrySideOutIndex':(0,0),\n",
    "                'sl': (0,0),\n",
    "                'spnnrOff': (0,0),\n",
    "               },\n",
    "     'y_col': 'gGLUr',\n",
    "     'name': 'base_simple'\n",
    "     },\n",
    "    {'X_cols': {\n",
    "                'photometryCenterInIndex':(0,0),\n",
    "#                 'photometryCenterOutIndex':(0,0),\n",
    "                'photometrySideInIndexAA':(0,0),\n",
    "                'photometrySideInIndexAa':(0,0),\n",
    "                'photometrySideInIndexaA':(0,0),\n",
    "                'photometrySideInIndexaa':(0,0),\n",
    "                'photometrySideInIndexAB':(0,0),\n",
    "                'photometrySideInIndexAb':(0,0),\n",
    "                'photometrySideInIndexaB':(0,0),\n",
    "                'photometrySideInIndexab':(0,0),\n",
    "                'photometrySideOutIndex':(0,0),\n",
    "                'sl': (0,0),\n",
    "                'spnnrOff': (0,0),\n",
    "               },\n",
    "     'y_col': 'gGLUr',\n",
    "     'name': 'base_words'},\n",
    "\n",
    "]]\n",
    "\n",
    "# Select hyper parameters for GLM to use for model selection\n",
    "# Step 1: Create a dictionary of lists for these relevant keywords...\n",
    "kwargs_iterations = {\n",
    "    # 'alpha': [0],\n",
    "    # 'l1_ratio': [0],\n",
    "\n",
    "#     'alpha': [0.0, 0.01, 0.1, 1.0],\n",
    "#     'l1_ratio': [0.0, 0.01, 0.1, 1.0],\n",
    "#     'alpha': [0.0, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0, 100000.0, 1000000.0],\n",
    "    'alpha': [0.0],\n",
    "    'l1_ratio': [0.0],\n",
    "}\n",
    "\n",
    "# Step 2: Create a dictionary for the fixed keyword arguments that do not require iteration...\n",
    "kwargs_fixed = {\n",
    "    'max_iter': 10000,\n",
    "    'fit_intercept': False\n",
    "}\n",
    "\n",
    "dfrr_cols = ['signal_file', 'file_num', 'nTrial', 'nTrial_filenum', 'nEndTrial', 'wi_trial_keep',\n",
    "#              'nTrial_hard', 'nEndTrial_hard', 'diffTrialNums_hard', 'wi_trial_keep_hard',\n",
    "             'has_all_cols', 'gDA', 'gACH', 'rDA', 'gGLUr', 'gGLUl',\n",
    "             'diffTrialNums', 'dupe',\n",
    "             'photometryCenterInIndex', 'photometryCenterOutIndex',\n",
    "             'photometrySideInIndexr', 'photometrySideInIndexnr',\n",
    "             'photometrySideOutIndex', 'spnnrOff', 'sl',\n",
    "\n",
    "             'photometrySideInIndexAA', 'photometrySideInIndexAa',\n",
    "             'photometrySideInIndexaA','photometrySideInIndexaa',\n",
    "             'photometrySideInIndexAB', 'photometrySideInIndexAb',\n",
    "             'photometrySideInIndexaB','photometrySideInIndexab',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb860145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate iterable list of keyword sets for possible combinations\n",
    "glm_kwarg_lst = sglm_cv.generate_mult_params(kwargs_iterations, kwargs_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399984d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1ce895",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 1\n",
    "param_set_output_path = output_path / Path(figname) / Path(groupid) / Path(param_set_id)\n",
    "param_set_output_path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "preproc_params = read_json(source_data_path / preproc_params_name)\n",
    "dump_dict_to_json(preproc_params, param_set_output_path / preproc_params_name)\n",
    "\n",
    "param_set_dct = {\n",
    "    'source_data_path': str((source_data_path).resolve()),\n",
    "    'output_path': str((output_path).resolve()),\n",
    "    'basis_name': basis_name,\n",
    "    'figname': figname,\n",
    "    'groupid': groupid,\n",
    "    'param_set_id': param_set_id,\n",
    "    'neg_order': neg_order,\n",
    "    'pos_order': pos_order,\n",
    "    'fix_training': fix_training,\n",
    "    'num_runs': num_runs,\n",
    "    'folds': folds,\n",
    "    'pholdout': pholdout,\n",
    "    'pgss': pgss,\n",
    "    'score_method': score_method,\n",
    "    'val_test_inx_sel_method': val_test_inx_sel_method,\n",
    "    'X_y_pairings_lst': X_y_pairings_lst,\n",
    "    'kwargs_iterations': kwargs_iterations,\n",
    "    'kwargs_fixed': kwargs_fixed,\n",
    "    'dfrr_cols': dfrr_cols,\n",
    "}\n",
    "dump_dict_to_json(param_set_dct, param_set_output_path / param_set_json_name)\n",
    "\n",
    "basis = pd.read_csv(source_data_path / basis_name, index_col=0)\n",
    "relevant_basis = basis[(basis['figname'] == figname)&(basis['groupid'] == groupid)]\n",
    "data_path = (source_data_path / figname / groupid)\n",
    "wt_used = basis['mouseid'].tolist()\n",
    "\n",
    "# Load Signal Data\n",
    "signal_files = []\n",
    "mouse_names = []\n",
    "for inx, basis_row in basis.iterrows():\n",
    "    signal_file_out = Path(basis_row['signal_file_out'])\n",
    "    signal_path = data_path / signal_file_out\n",
    "    signal_files.append(str(signal_path.resolve()))\n",
    "    mouse_names.append(basis_row['mouseid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f9e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3875a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c2d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb61ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_width = 2\n",
    "# plot_rows_lst = [_//plot_width + (_%plot_width > 0)*1 for _ in max_cols_len_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b248ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_ids ['GLM_SIGNALS_INTERIM_S1299_02022022.csv', 'GLM_SIGNALS_INTERIM_S1299_02072022.csv', 'GLM_SIGNALS_INTERIM_S1300_02022022.csv', 'GLM_SIGNALS_INTERIM_S1300_02072022.csv', 'GLM_SIGNALS_INTERIM_S1301_01312022.csv', 'GLM_SIGNALS_INTERIM_S1301_02022022.csv', 'GLM_SIGNALS_INTERIM_S1302_01312022.csv', 'GLM_SIGNALS_INTERIM_S1302_02022022.csv']\n",
      "{'max_iter': 10000, 'fit_intercept': False, 'alpha': 0.0, 'l1_ratio': 0.0}\n",
      "Running multi\n",
      "Running multi\n",
      "Running multi\n",
      "Running multi\n",
      "Multirun queue completed\n",
      "Multirun queue completedMultirun queue completed\n",
      "\n",
      "{'max_iter': 10000, 'fit_intercept': False, 'alpha': 0.0, 'l1_ratio': 0.0}\n",
      "> cv_mean_score_train: 0.3731476586264627\n",
      "> cv_R2_score: 0.37085583843941394\n",
      "> cv_mean_score: 0.37102745179542324\n",
      "Running multi\n",
      "Multirun queue completed\n",
      "Variable Sizes:\n",
      "      dfrel_basis_has_all_cols:  1.6 GiB\n",
      "                   dfrel_basis:  1.6 GiB\n",
      "                      df_train: 625.5 MiB\n",
      "                    df_holdout: 610.4 MiB\n",
      "                       X_train: 224.2 MiB\n",
      "                     X_holdout: 218.7 MiB\n",
      "                  holdout_irun:  2.6 MiB\n",
      "                  holdout_iXyd:  2.6 MiB\n",
      "              holdout_igkwargs:  2.6 MiB\n",
      "                       y_train:  1.8 MiB\n",
      "                     y_holdout:  1.8 MiB\n",
      "                    holdout_df: 899.1 KiB\n",
      "                  has_all_cols: 299.8 KiB\n",
      "                       holdout: 299.8 KiB\n",
      "                          _i11: 15.0 KiB\n",
      "                         basis:  5.0 KiB\n",
      "                relevant_basis:  5.0 KiB\n",
      "                   X_cols_sftd:  4.6 KiB\n",
      "                           _i7:  3.0 KiB\n",
      "                        X_cols:  2.0 KiB\n",
      "                           _i5:  2.0 KiB\n",
      "                           _ii:  1.6 KiB\n",
      "                           _i9:  1.6 KiB\n",
      "                     basis_row:  1.5 KiB\n",
      "                          HTML:  1.0 KiB\n",
      "                          Path:  904.0 B\n",
      "                           _i1:  830.0 B\n",
      "                           _i6:  798.0 B\n",
      "                 param_set_dct:  640.0 B\n",
      "                 widest_orders:  640.0 B\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:>30}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{:>8}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(vname, sizeof_fmt(size)))\n\u001b[0;32m    120\u001b[0m glm, holdout_score, holdout_neg_mse_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_fit_holdout_score(X_train, y_train, X_holdout, y_holdout, best_params)\n\u001b[0;32m    122\u001b[0m betas_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mconcatenate([np\u001b[38;5;241m.\u001b[39marray([glm\u001b[38;5;241m.\u001b[39mintercept_]), glm\u001b[38;5;241m.\u001b[39mcoef_], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m--> 123\u001b[0m             index\u001b[38;5;241m=\u001b[39m[\u001b[43mrun_id\u001b[49m], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39mX_cols)\n\u001b[0;32m    124\u001b[0m betas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmouse_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m mouse_id\n\u001b[0;32m    125\u001b[0m betas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miXyd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_run_num=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mirun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_id' is not defined"
     ]
    }
   ],
   "source": [
    "# Part 2: Generate base files to use by all runs and create folders for each run\n",
    "\n",
    "max_cols_len_lst = [max([len(_['X_cols']) for _ in inner_list]) for inner_list in X_y_pairings_lst]\n",
    "multi_start = time.time()\n",
    "for iXyp, X_y_pairings in enumerate(X_y_pairings_lst):\n",
    "\n",
    "    widest_orders = smf.xy_pairs_to_widest_orders([{'X_cols': smf.X_cols_dict_to_default(_['X_cols'], neg_order, pos_order),\n",
    "                                                    'y_col': _['y_col']} for _ in X_y_pairings])\n",
    "    max_cols_len = max_cols_len_lst[iXyp]\n",
    "    mouse_names, combo_dfs, combo_fns, X_cols_sftd = extract_multifiles(wt_used, signal_files, widest_orders)\n",
    "    \n",
    "    start = time.time()\n",
    "    results_dict = {}\n",
    "    \n",
    "    dfrel_basis = combo_dfs[0].reset_index(drop=False).copy()\n",
    "    \n",
    "#     fn = Path(combo_fns[file_num].split('.')[0]).parts[-1]\n",
    "    \n",
    "    mouse_id = mouse_names[0]\n",
    "    dfresids_cols = np.copy(dfrr_cols).tolist()\n",
    "#     run_id = f'{fn}_{iXyp}'         \n",
    "    has_all_cols = id_rows_with_all_cols(dfrel_basis, X_y_pairings, X_cols_sftd)\n",
    "    if has_all_cols.sum() == 0:\n",
    "        print(f'No datapoints found for non-NaN dropcols & non-zero ycols for fixed_training: {prefix}_{fn}')\n",
    "        continue\n",
    "\n",
    "    dfrel_basis['has_all_cols'] = has_all_cols\n",
    "    \n",
    "    Xyp_path = param_set_output_path / Path(f'iXyp_{iXyp}')\n",
    "    Xyp_path.mkdir(parents=True, exist_ok=False)\n",
    "    dump_dict_to_json(X_y_pairings, Xyp_path / Path('X_y_pairings.json'))\n",
    "    \n",
    "#     dfrel_basis[dfresids_cols].set_index(['nTrial_filenum'], append=True).to_hdf(str((Xyp_path / Path(f'combo_df.h5')).resolve()), key='combo', index=True,)\n",
    "    dfrel_basis[dfresids_cols].set_index(['nTrial_filenum'], append=True).to_csv(str((Xyp_path / Path(f'combo_df.csv')).resolve()), index=True)\n",
    "    \n",
    "    dfrel_basis_has_all_cols = dfrel_basis[dfrel_basis['has_all_cols']]\n",
    "    \n",
    "    cv_holdout_idx_params = dict(df_available=dfrel_basis_has_all_cols,\n",
    "                                 pholdout=pholdout,\n",
    "                                 folds=folds,\n",
    "                                 pgss=pgss,\n",
    "                                 filter_series=dfrel_basis_has_all_cols['wi_trial_keep'],\n",
    "                                 id_cols=['nTrial_filenum'])\n",
    "    \n",
    "    for irun in range(num_runs):\n",
    "        holdout_irun, cv_idx_irun = get_holdout_and_cv_idx(**cv_holdout_idx_params)\n",
    "        \n",
    "        for iXyd, X_y_dct in enumerate(X_y_pairings):\n",
    "            holdout_iXyd, cv_idx_iXyd = get_holdout_and_cv_idx(**cv_holdout_idx_params)\n",
    "            \n",
    "            for igkwargs, glm_kwargs in enumerate(glm_kwarg_lst):\n",
    "                holdout_igkwargs, cv_idx_igkwargs = get_holdout_and_cv_idx(**cv_holdout_idx_params)\n",
    "                \n",
    "                run_path = Xyp_path / f'iXyd_{iXyd}-ikwargs_{igkwargs}' / f'irun_{irun}'\n",
    "                run_path.mkdir(parents=True, exist_ok=False)\n",
    "                \n",
    "                specific_model_params = {\n",
    "                    'irun': irun,\n",
    "                    'iXyd': iXyd,\n",
    "                    'igkwargs': igkwargs,\n",
    "                    'X_y_dct': X_y_dct,\n",
    "                    'glm_kwargs': glm_kwargs,\n",
    "                }\n",
    "                dump_dict_to_json(specific_model_params, run_path / Path('specific_model_params.json'))\n",
    "                dump_dict_to_json(param_set_dct, run_path / param_set_json_name)\n",
    "                \n",
    "                holdout_df = pd.DataFrame({'holdout_irun': holdout_irun,\n",
    "                                           'holdout_iXyd':holdout_iXyd,\n",
    "                                           'holdout_igkwargs':holdout_igkwargs},\n",
    "                                          index=dfrel_basis.index)\n",
    "                \n",
    "#                 holdout_df.to_hdf(str((run_path / 'holdout_options.hd5').resolve()), key='holdout')\n",
    "                \n",
    "                cv_options_dct = {\n",
    "                    'cv_idx_irun':cv_idx_irun,\n",
    "                    'cv_idx_iXyd':cv_idx_iXyd,\n",
    "                    'cv_idx_igkwargs':cv_idx_igkwargs,\n",
    "                }\n",
    "#                 np.save(str((run_path/'cv_options.npy').resolve()), np.array(cv_options_dct, dtype='object'))\n",
    "                \n",
    "                kfold_cv_idx = cv_options_dct[f'cv_idx_{val_test_inx_sel_method}']\n",
    "                holdout = holdout_df[f'holdout_{val_test_inx_sel_method}']\n",
    "                \n",
    "                np.save(str((run_path/'kfold_cv_idx.npy').resolve()), np.array(kfold_cv_idx, dtype='object'))\n",
    "                holdout.to_hdf(str((run_path / 'holdout.hd5').resolve()), key='holdout')\n",
    "                \n",
    "                \n",
    "                \n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "                #### JZ TODO: MOVE TO SEPARATE FILE FOR PARALLELIZATION OVER DEEPEST FOLDERS...\n",
    "\n",
    "                X_cols = bf.col_shift_bounds_dict_to_col_list(X_y_dct['X_cols'], X_cols_sftd)\n",
    "                y_col = X_y_dct['y_col']\n",
    "                name = X_y_dct['name']\n",
    "\n",
    "\n",
    "                df_train = dfrel_basis_has_all_cols[(~holdout)&(dfrel_basis_has_all_cols['wi_trial_keep'])]\n",
    "                df_holdout = dfrel_basis_has_all_cols[(holdout)&(dfrel_basis_has_all_cols['wi_trial_keep'])]\n",
    "\n",
    "                X_train, y_train, X_holdout, y_holdout = df_train[X_cols], df_train[y_col], df_holdout[X_cols], df_holdout[y_col]\n",
    "                best_score, best_score_std, best_params, best_model, cv_results = models.sglm_cv.simple_cv_fit(X_train, y_train, kfold_cv_idx, [glm_kwargs], model_type='Normal',\n",
    "                                                                                                            verbose=0, score_method=score_method)\n",
    "\n",
    "\n",
    "                print('Variable Sizes:')\n",
    "                for vname, size in sorted(((vname, sys.getsizeof(value)) for vname, value in locals().items()),\n",
    "                                        key= lambda x: -x[1])[:30]:\n",
    "                    print(\"{:>30}: {:>8}\".format(vname, sizeof_fmt(size)))\n",
    "\n",
    "                glm, holdout_score, holdout_neg_mse_score = eval.training_fit_holdout_score(X_train, y_train, X_holdout, y_holdout, best_params)\n",
    "\n",
    "                betas_df = pd.DataFrame(np.concatenate([np.array([glm.intercept_]), glm.coef_], axis=0).reshape(1,-1),\n",
    "                            index=[run_id], columns=['int']+X_cols)\n",
    "                betas_df['mouse_id'] = mouse_id\n",
    "                betas_df['channel_name'] = f'{y_col}_{iXyd}_run_num={irun}'\n",
    "                betas_df['name'] = name\n",
    "\n",
    "                assert len(cv_results['full_cv_results']) == 1\n",
    "                assert holdout_neg_mse_score == glm.neg_mse_score(X_holdout, y_holdout)\n",
    "\n",
    "                betas_df[['mse_tr', 'mse_cv', 'mse_te']] = [[-glm.neg_mse_score(X_train, y_train), cv_results['full_cv_results'][0]['cv_mse_score'], -glm.neg_mse_score(X_holdout, y_holdout)]]\n",
    "                betas_df[['r2_tr', 'r2_cv', 'r2_te']] = [[glm.r2_score(X_train, y_train), cv_results['full_cv_results'][0]['cv_R2_score'], glm.r2_score(X_holdout, y_holdout)]]\n",
    "\n",
    "                multi_end = time.time()\n",
    "                time_passed = str(multi_end - multi_start) + ' s'\n",
    "                betas_df['timestamp'] = time_passed\n",
    "                print('run_id, subrun_id', run_id, subrun_id, f'- Time Passed: {time_passed}')\n",
    "\n",
    "                betas_df = betas_df.set_index(['mouse_id', 'channel_name', 'name', 'timestamp', 'mse_tr', 'mse_cv', 'mse_te', 'r2_tr', 'r2_cv', 'r2_te'], append=True)\n",
    "\n",
    "                word_perc_kwargs = dict(total_col='photometrySideInIndex', words_prefix='photometrySideInIndex', words=['AA', 'Aa', 'aA', 'aa', 'AB', 'Ab', 'aB', 'ab'])\n",
    "                betas_df = get_perc_words(df_train, betas_df, perc_suffix='_cnt_tr',  **word_perc_kwargs)\n",
    "                betas_df = get_perc_words(df_holdout, betas_df, perc_suffix='_cnt_ho', **word_perc_kwargs)\n",
    "\n",
    "                betas_df.to_hdf(pr(f'{base_folder}/{prefix}/{best_coefs_folder}/{run_id}_{hyp_str}_best_coeffs.h5'), key=subrun_id, index=True)\n",
    "\n",
    "                dfrel_basis['pred_'+pred_col_name] = pd.Series(glm.predict(dfrel_basis_has_all_cols[X_cols]),\n",
    "                                                                            index=dfrel_basis_has_all_cols.index)\n",
    "                dfrel_basis['predALL_'+pred_col_name] = glm.predict(dfrel_basis[X_cols])\n",
    "\n",
    "                subset_dfresids_cols = [f'holdout_{pred_col_name}', 'pred_'+pred_col_name, 'predALL_'+pred_col_name]\n",
    "                dfresids_cols += subset_dfresids_cols\n",
    "                dfrel_basis[dfresids_cols].set_index(['nTrial_filenum'], append=True)[subset_dfresids_cols].to_hdf(pr(f'{base_folder}/{prefix}/{best_reconstruct_folder}/best_resids_{run_id}.h5'), key=subrun_id, index=True,)\n",
    "\n",
    "                \n",
    "                \n",
    "                break\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "    \n",
    "                \n",
    "    \n",
    "#     dfrel_basis[dfresids_cols].set_index(['nTrial_filenum'], append=True).to_hdf(pr(f'{base_folder}/{prefix}/{best_reconstruct_folder}/best_resids_{run_id}.h5'), key='dfrel_basis', index=True,)\n",
    "#     dfrel_basis_has_all_cols = dfrel_basis[has_all_cols]\n",
    "    \n",
    "    \n",
    "#     # Define on a Per Run Basis\n",
    "#     holdout = models.split_data.holdout_split_by_trial_id(dfrel_basis_has_all_cols,\n",
    "#                                                         id_cols=['nTrial_filenum'],\n",
    "#                                                         perc_holdout=pholdout)\n",
    "#     dfrel_basis[f'holdout_iXyp={iXyp}_irun={irun}'] = holdout\n",
    "#     dfrel_basis[f'holdout_iXyp={iXyp}_irun={irun}'] = dfrel_basis[f'holdout_iXyp={iXyp}_irun={irun}'].astype(float)\n",
    "    \n",
    "    \n",
    "#     for iXyd, X_y_dct in enumerate(X_y_pairings):\n",
    "\n",
    "#         for glm_kwargs in glm_kwarg_lst:\n",
    "#             print(glm_kwargs)\n",
    "#             hyp_str = '__'.join([''.join([str(__) for __ in _]) for _ in glm_kwargs.items()]).replace('.', '____')\n",
    "\n",
    "\n",
    "#             X_cols = bf.col_shift_bounds_dict_to_col_list(X_y_dct['X_cols'], X_cols_sftd)\n",
    "#             y_col = X_y_dct['y_col']\n",
    "#             name = X_y_dct['name']\n",
    "\n",
    "#             subrun_id = f'{y_col}_{iXyp}_{iXyd}_{name}_{hyp_str}_run_num__{irun}'\n",
    "\n",
    "#             pred_col_name = subrun_id\n",
    "\n",
    "\n",
    "#             if not fix_training:\n",
    "#                 holdout = models.split_data.holdout_split_by_trial_id(dfrel_basis_has_all_cols, id_cols=['nTrial_filenum'], perc_holdout=pholdout)\n",
    "#                 dfrel_basis[f'holdout_{pred_col_name}'] = holdout\n",
    "#                 dfrel_basis[f'holdout_{pred_col_name}'] = dfrel_basis[f'holdout_{pred_col_name}'].astype(float)\n",
    "#             else:\n",
    "#                 dfrel_basis[f'holdout_{pred_col_name}'] = dfrel_basis[f'holdout_iXyp={iXyp}_irun={irun}'].astype(float)\n",
    "\n",
    "#             if (~dfrel_basis[f'holdout_{pred_col_name}'].isna()).sum() == 0:\n",
    "#                 print(f'No datapoints found for non-NaN dropcols & non-zero ycols for run id: {run_id}, {subrun_id}.')\n",
    "#                 continue\n",
    "\n",
    "#             df_train = dfrel_basis_has_all_cols[(~holdout)&(dfrel_basis_has_all_cols['wi_trial_keep'])]\n",
    "#             df_holdout = dfrel_basis_has_all_cols[(holdout)&(dfrel_basis_has_all_cols['wi_trial_keep'])]\n",
    "\n",
    "#             X_train, y_train, X_holdout, y_holdout = df_train[X_cols], df_train[y_col], df_holdout[X_cols], df_holdout[y_col]\n",
    "\n",
    "#             kfold_cv_idx = models.split_data.cv_idx_by_trial_id(df_train, trial_id_columns=['nTrial_filenum'], num_folds=folds, test_size=pgss)\n",
    "\n",
    "#             best_score, best_score_std, best_params, best_model, cv_results = models.sglm_cv.simple_cv_fit(X_train, y_train, kfold_cv_idx, [glm_kwargs], model_type='Normal',\n",
    "#                                                                                                         verbose=0, score_method=score_method)\n",
    "\n",
    "\n",
    "#             print('Variable Sizes:')\n",
    "#             for vname, size in sorted(((vname, sys.getsizeof(value)) for vname, value in locals().items()),\n",
    "#                                     key= lambda x: -x[1])[:30]:\n",
    "#                 print(\"{:>30}: {:>8}\".format(vname, sizeof_fmt(size)))\n",
    "\n",
    "#             glm, holdout_score, holdout_neg_mse_score = eval.training_fit_holdout_score(X_train, y_train, X_holdout, y_holdout, best_params)\n",
    "\n",
    "#             betas_df = pd.DataFrame(np.concatenate([np.array([glm.intercept_]), glm.coef_], axis=0).reshape(1,-1),\n",
    "#                         index=[run_id], columns=['int']+X_cols)\n",
    "#             betas_df['mouse_id'] = mouse_id\n",
    "#             betas_df['channel_name'] = f'{y_col}_{iXyd}_run_num={irun}'\n",
    "#             betas_df['name'] = name\n",
    "\n",
    "#             assert len(cv_results['full_cv_results']) == 1\n",
    "#             assert holdout_neg_mse_score == glm.neg_mse_score(X_holdout, y_holdout)\n",
    "\n",
    "#             betas_df[['mse_tr', 'mse_cv', 'mse_te']] = [[-glm.neg_mse_score(X_train, y_train), cv_results['full_cv_results'][0]['cv_mse_score'], -glm.neg_mse_score(X_holdout, y_holdout)]]\n",
    "#             betas_df[['r2_tr', 'r2_cv', 'r2_te']] = [[glm.r2_score(X_train, y_train), cv_results['full_cv_results'][0]['cv_R2_score'], glm.r2_score(X_holdout, y_holdout)]]\n",
    "\n",
    "#             multi_end = time.time()\n",
    "#             time_passed = str(multi_end - multi_start) + ' s'\n",
    "#             betas_df['timestamp'] = time_passed\n",
    "#             print('run_id, subrun_id', run_id, subrun_id, f'- Time Passed: {time_passed}')\n",
    "\n",
    "#             betas_df = betas_df.set_index(['mouse_id', 'channel_name', 'name', 'timestamp', 'mse_tr', 'mse_cv', 'mse_te', 'r2_tr', 'r2_cv', 'r2_te'], append=True)\n",
    "\n",
    "#             word_perc_kwargs = dict(total_col='photometrySideInIndex', words_prefix='photometrySideInIndex', words=['AA', 'Aa', 'aA', 'aa', 'AB', 'Ab', 'aB', 'ab'])\n",
    "#             betas_df = get_perc_words(df_train, betas_df, perc_suffix='_cnt_tr',  **word_perc_kwargs)\n",
    "#             betas_df = get_perc_words(df_holdout, betas_df, perc_suffix='_cnt_ho', **word_perc_kwargs)\n",
    "\n",
    "#             betas_df.to_hdf(pr(f'{base_folder}/{prefix}/{best_coefs_folder}/{run_id}_{hyp_str}_best_coeffs.h5'), key=subrun_id, index=True)\n",
    "\n",
    "#             dfrel_basis['pred_'+pred_col_name] = pd.Series(glm.predict(dfrel_basis_has_all_cols[X_cols]),\n",
    "#                                                                         index=dfrel_basis_has_all_cols.index)\n",
    "#             dfrel_basis['predALL_'+pred_col_name] = glm.predict(dfrel_basis[X_cols])\n",
    "\n",
    "#             subset_dfresids_cols = [f'holdout_{pred_col_name}', 'pred_'+pred_col_name, 'predALL_'+pred_col_name]\n",
    "#             dfresids_cols += subset_dfresids_cols\n",
    "#             dfrel_basis[dfresids_cols].set_index(['nTrial_filenum'], append=True)[subset_dfresids_cols].to_hdf(pr(f'{base_folder}/{prefix}/{best_reconstruct_folder}/best_resids_{run_id}.h5'), key=subrun_id, index=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335648f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2371f578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
