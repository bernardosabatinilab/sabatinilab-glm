{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sglm.models import sglm\n",
    "from sglm.features import gen_signal_df as gsd\n",
    "from sglm.features import build_features as bf\n",
    "from sglm.features import gen_signal_df as gsd\n",
    "from sglm.features import build_features as bf\n",
    "\n",
    "import itertools\n",
    "\n",
    "num_sft = (-20, 20)\n",
    "channel_definitions = {}\n",
    "signal_files = []\n",
    "out_files = {}\n",
    "ignore_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_col_lst_all = ['gACH', 'rDA', 'gDA', 'gGLU', 'rGLU',\n",
    "                 'gACH_flx_drd', 'rDA_flx_drd', 'gDA_flx_drd',\n",
    "                 'gACH_flx_cht', 'rDA_flx_cht', 'gDA_flx_cht',\n",
    "                 'gACH_flx_drdcht', 'rDA_flx_drdcht', 'gDA_flx_drdcht',\n",
    "                 'Ch1', 'Ch2', 'Ch5', 'Ch6',\n",
    "                 'GP_1', 'GP_2', 'GP_5', 'GP_6', 'SGP_1', 'SGP_2', 'SGP_5', 'SGP_6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from logging.config import _RootLoggerConfiguration\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def create_folder_if_not_exists(dr):\n",
    "    dr = Path(dr)\n",
    "    constructed_dir = str(Path('/').resolve())\n",
    "    made = False\n",
    "    \n",
    "    # print('constructed_dir', constructed_dir)\n",
    "    for fold in dr.parts:\n",
    "        if len(fold) == 0:\n",
    "            continue\n",
    "        constructed_dir = str((Path(constructed_dir) / fold).resolve())\n",
    "#         print('constructed_dir', constructed_dir)\n",
    "        if os.path.isdir(constructed_dir):\n",
    "            # print(f'Directory already exists:', constructed_dir)\n",
    "            pass\n",
    "        else:\n",
    "            # print(f'Creating directory:', constructed_dir)\n",
    "            os.mkdir(constructed_dir)\n",
    "            made = True\n",
    "    if made:\n",
    "        print(f'Created directory:', constructed_dir)\n",
    "    return\n",
    "\n",
    "# create_folder_if_not_exists((Path.home() / 'Desktop/nada/folder2').resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ### Figure 1: Single side recording\n",
    "# figure = 'Figure_1_2'\n",
    "\n",
    "# # # Load Signal Data\n",
    "\n",
    "# # signal_files = glob.glob(f'../../data/raw-new/GLM_SIGNALS_WT61_*')\n",
    "# # signal_files += glob.glob(f'../../data/raw-new/GLM_SIGNALS_WT63_*')\n",
    "# # signal_files += glob.glob(f'../../data/raw-new/GLM_SIGNALS_WT64_*')\n",
    "\n",
    "# ignore_files += [\n",
    "#                 'WT61_10152021',\n",
    "#                 'WT61_10082021'\n",
    "#                 ]\n",
    "# # for ign in ignore_files:\n",
    "# #     signal_files = [_ for _ in signal_files if ign not in _]\n",
    "\n",
    "# # table_files = [_.replace('GLM_SIGNALS', 'GLM_TABLE') for _ in signal_files]\n",
    "\n",
    "# # channel_definitions = {\n",
    "# #         ('WT61',): {'Ch1': 'gACH', 'Ch2': 'rDA'},\n",
    "# #         ('WT64',): {'Ch1': 'gACH', 'Ch2': 'empty'},\n",
    "# #         ('WT63',): {'Ch1': 'gDA', 'Ch2': 'empty'},\n",
    "# #     }\n",
    "\n",
    "# group_1_mice = ['WT63', 'WT64', 'WT65']\n",
    "# # group_1_sess = ['11082021', '11102021', '11122021', '11182021']\n",
    "# group_1_sess = ['11082021', '11102021', '11122021', '11162021', '11182021']\n",
    "# group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "# group_2_mice = ['WT66', 'WT67', 'WT68', 'WT69']\n",
    "# group_2_sess = ['12132021', '12152021', '12172021', '12192021']\n",
    "# group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "# group_3_mice = ['WT58', 'WT60', 'WT61']\n",
    "# group_3_sess = ['10042021', '10062021', '10082021', '10112021', '10132021', '10152021']\n",
    "# group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "# group_4_mice = ['WT53', 'WT54', 'WT55', 'WT56']\n",
    "# group_4_sess = ['09012021', '09032021', '09062021']\n",
    "# group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "# group_5_mice = ['WT57', 'WT59']\n",
    "# group_5_sess = ['10042021', '10062021', '10082021', '10112021', '10132021', '10152021']\n",
    "# group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "# group_6_mice = []#['WT62']\n",
    "# group_6_sess = []#['11082021', '11102021', '11122021', '11182021']\n",
    "# group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# channel_definitions = {}\n",
    "# # channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "# channel_definitions.update({(file_combo,): {'Ch1': 'gDA', 'Ch5': 'gACH'} for file_combo in group_1_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch1': 'gDA'} for file_combo in group_2_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch1': 'gACH'} for file_combo in group_3_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch1': 'gACH'} for file_combo in group_4_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH'} for file_combo in group_5_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch1': 'gDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig_1_signal_files_setup = []#group_1_combo + group_2_combo + group_3_combo + group_4_combo + group_5_combo + group_6_combo\n",
    "\n",
    "# for f in fig_1_signal_files_setup:\n",
    "#     glob_file = glob.glob(str(Path(f'../../data/raw-new/{figure}/GLM_SIGNALS_{f}*').resolve()))\n",
    "#     if len(glob_file) != 1:\n",
    "#         print('Missing file!!! ', f)\n",
    "#     signal_files += glob_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Figure 3: Dual side recording\n",
    "# figure = 'Figure_3'\n",
    "\n",
    "# # 'WT61', 'WT63', 'WT64', '', 'WT44', 'WT51' # Excluding 'WT43\n",
    "\n",
    "# # ['03162021','03192021','03232021','03262021','07282021','07302021','08012021', '08042021', '08062021', '08102021', '10042021',\n",
    "# # '10082021', '10112021', '10132021', '10182021', '11082021', '11102021', '11122021', '11162021', '11182021']\n",
    "\n",
    "# group_1_mice = ['WT63', 'WT64', 'WT44', 'WT51']\n",
    "# group_1_sess = ['03162021','03192021','03232021','03262021','07282021','07302021','08012021', '08042021', '08062021', '08102021', '10042021',\n",
    "#                 '10082021', '10112021', '10132021', '10182021', '11082021', '11102021', '11122021', '11162021', '11182021']\n",
    "# group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "# group_2_mice = ['WT61']\n",
    "# group_2_sess = ['03162021','03192021','03232021','03262021','07282021','07302021','08012021', '08042021', '08062021', '08102021', '10042021',\n",
    "#                 '10082021', '10112021', '10132021', '11082021', '11102021', '11122021', '11162021', '11182021']\n",
    "# group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "# # group_3_mice = ['WT58', 'WT60', 'WT61']\n",
    "# # group_3_sess = ['10042021', '10062021', '10082021', '10112021', '10132021', '10152021']\n",
    "# # group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "# # group_4_mice = ['WT53', 'WT54', 'WT55', 'WT56']\n",
    "# # group_4_sess = ['09012021', '09032021', '09062021']\n",
    "# # group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "# # group_5_mice = ['WT61']\n",
    "# # group_5_sess = ['10042021', '10062021', '10082021', '10112021', '10132021', '10152021']\n",
    "# # group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "# group_6_mice = ['WT43']\n",
    "# group_6_sess = []\n",
    "# group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# # channel_definitions = {}\n",
    "# # channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_1_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_2_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch1': 'gACH'} for file_combo in group_3_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch1': 'gACH'} for file_combo in group_4_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "# fig_3_signal_files_setup = []#group_1_combo + group_2_combo #+ group_3_combo + group_4_combo + group_5_combo + group_6_combo\n",
    "# for f in fig_3_signal_files_setup:\n",
    "#     glob_file = glob.glob(str(Path(f'../../data/raw-new/{figure}/GLM_SIGNALS_{f}*').resolve()))\n",
    "#     if len(glob_file) != 1:\n",
    "#         print('Missing file!!! ', f)\n",
    "#     signal_files += glob_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Figure 3 dualhem\n",
    "# figure = 'Figure3-dualhem'\n",
    "\n",
    "# group_1_mice = ['WT62', 'WT63', 'WT64', 'WT65']\n",
    "# group_1_sess = ['10042021', '10082021', '10112021', '10132021',\n",
    "#                 '10182021', '11082021', '11102021', '11122021', '11162021', '11182021']\n",
    "# group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "# group_2_mice = []\n",
    "# group_2_sess = []\n",
    "# group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "# group_3_mice = []\n",
    "# group_3_sess = []\n",
    "# group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "# group_4_mice = []\n",
    "# group_4_sess = []\n",
    "# group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "# group_5_mice = []\n",
    "# group_5_sess = []\n",
    "# group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "# group_6_mice = []\n",
    "# group_6_sess = []\n",
    "# group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# # channel_definitions = {}\n",
    "# # channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_1_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_2_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_3_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_4_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "# fig_3dh_signal_files_setup = [] #[group_1_combo, group_2_combo, group_3_combo, group_4_combo, group_5_combo, group_6_combo]\n",
    "# for g_num in range(0, len(fig_3dh_signal_files_setup)):\n",
    "#     fig_3dh_basis = fig_3dh_signal_files_setup[g_num]\n",
    "#     for f in fig_3dh_basis:\n",
    "#         glob_file = glob.glob(str(Path(f'../../data/raw-new/{figure}/GLM_SIGNALS_{f}*').resolve()))\n",
    "#         if len(glob_file) != 1:\n",
    "#             print('Missing file!!! ', f)\n",
    "#         else:\n",
    "#             signal_files += glob_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Figure 5: Flox Mice\n",
    "# figure = 'Figure_5'\n",
    "\n",
    "# # all_session_nums = [\n",
    "# #                     '03172022', '03212022', '03232022', '03252022', '03292022', '03312022', '04032022', '04052022',\n",
    "# #                     '04072022', '04082022', '04112022', '04132022', '04152022', '04202022', '04222022', '05022022',\n",
    "# #                     '05102022', '05162022', '05182022', '05222022', '05232022', '05242022', '05252022', '05262022',\n",
    "# #                     '05272022', '05292022', '05302022', '05312022',\n",
    "# #                     ]\n",
    "# all_session_nums = [\n",
    "#                     '03172022', '03212022', '03232022', '03252022', '03282022',\n",
    "#                     '03292022', '03302022', '03312022', '04012022', '04032022',\n",
    "#                     '04052022', '04072022', '04082022', '04112022', '04132022',\n",
    "#                     '04152022', '04202022', '04222022', '05022022', '05102022',\n",
    "#                     '05162022', '05182022', '05222022', '05232022', '05242022',\n",
    "#                     '05252022', '05262022', '05272022', '05292022', '05302022',\n",
    "#                     '05312022', '06022022', '06062022', '06082022', '06102022',\n",
    "#                     '06132022', '06152022', '06182022', '07072022', '07082022',\n",
    "#                     '07092022', '07112022', '07122022', '07132022', '07142022',\n",
    "#                     '07152022', '07162022', '07172022', '07182022', '07192022'\n",
    "#                    ]\n",
    "\n",
    "# # Drd2f/f control: S1417, 1419, 1421\n",
    "# # group_1_mice = ['S1417', 'S1419', 'S1421']\n",
    "# group_1_mice = ['S1417', 'S1419', 'S1421', 'S1460', 'S1462', 'S1473', 'S1474']\n",
    "# group_1_sess = all_session_nums\n",
    "# group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "# # Chat Cre X Drd2f/f: S1416, 1418, 1420, 1422\n",
    "# # group_2_mice = ['S1416', 'S1418', 'S1420', 'S1422']\n",
    "# group_2_mice = ['S1416', 'S1418', 'S1420', 'S1459', 'S1461', 'S1470', 'S1471', 'S1472'] # 'S1422', \n",
    "# group_2_sess = all_session_nums\n",
    "# group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "# # Chat Cre control: S1355-1358, S1376\n",
    "# # group_3_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1374', 'S1376']\n",
    "# group_3_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1376',\n",
    "#                 'S1448', 'S1449', 'S1450', 'S1451']\n",
    "# group_3_sess = all_session_nums\n",
    "# group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "# # Chat Cre control: S1399-1401\n",
    "# group_4_mice = ['S1399', 'S1400', 'S1401']\n",
    "# group_4_sess = all_session_nums\n",
    "# group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "# # group_5_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1374', 'S1376',\n",
    "# #                'S1448', 'S1449', 'S1450', 'S1451'\n",
    "# #                'S1399', 'S1400', 'S1401']\n",
    "# group_5_mice = []\n",
    "# group_5_sess = all_session_nums\n",
    "# group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "# group_6_mice = []\n",
    "# group_6_sess = []\n",
    "# group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# # channel_definitions = {}\n",
    "# # channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_1_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_2_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_3_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_4_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "# out_files = {}\n",
    "\n",
    "# fig_5_signal_files_setup = [group_1_combo, group_2_combo, group_3_combo, group_4_combo, group_5_combo, group_6_combo]\n",
    "# for g_num in range(0, len(fig_5_signal_files_setup)):\n",
    "#     fig_5_basis = fig_5_signal_files_setup[g_num]\n",
    "#     for f in fig_5_basis:\n",
    "#         glob_file = glob.glob(str(Path(f'../../data/raw-new/{figure}/GLM_SIGNALS_{f}*').resolve()))\n",
    "#         if len(glob_file) != 1:\n",
    "#             print('Missing file!!! ', f)\n",
    "#         else:\n",
    "#             signal_files += glob_file\n",
    "#             out_files[glob_file[0]] = (figure, figure + '/g' + str(g_num+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Figure 5: Flox Mice\n",
    "# figure = 'fig5'\n",
    "\n",
    "# # all_session_nums = [\n",
    "# #                     '03172022', '03212022', '03232022', '03252022', '03292022', '03312022', '04032022', '04052022',\n",
    "# #                     '04072022', '04082022', '04112022', '04132022', '04152022', '04202022', '04222022', '05022022',\n",
    "# #                     '05102022', '05162022', '05182022', '05222022', '05232022', '05242022', '05252022', '05262022',\n",
    "# #                     '05272022', '05292022', '05302022', '05312022',\n",
    "# #                     ]\n",
    "# all_session_nums = [\n",
    "#                     '03172022', '03212022', '03232022', '03252022', '03282022',\n",
    "#                     '03292022', '03302022', '03312022', '04012022', '04032022',\n",
    "#                     '04052022', '04072022', '04082022', '04112022', '04132022',\n",
    "#                     '04152022', '04202022', '04222022', '05022022', '05102022',\n",
    "#                     '05162022', '05182022', '05222022', '05232022', '05242022',\n",
    "#                     '05252022', '05262022', '05272022', '05292022', '05302022',\n",
    "#                     '05312022', '06022022', '06062022', '06082022', '06102022',\n",
    "#                     '06132022', '06152022', '06182022', '07072022', '07082022',\n",
    "#                     '07092022', '07112022', '07122022', '07132022', '07142022',\n",
    "#                     '07152022', '07162022', '07172022', '07182022', '07192022'\n",
    "#                    ]\n",
    "\n",
    "# # Drd2f/f control: S1417, 1419, 1421\n",
    "# # group_1_mice = ['S1417', 'S1419', 'S1421']\n",
    "# group_1_mice = ['S1417', 'S1419', 'S1421', 'S1460', 'S1462', 'S1473', 'S1474']\n",
    "# group_1_sess = all_session_nums\n",
    "# group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "# # Chat Cre X Drd2f/f: S1416, 1418, 1420, 1422\n",
    "# # group_2_mice = ['S1416', 'S1418', 'S1420', 'S1422']\n",
    "# group_2_mice = ['S1416', 'S1418', 'S1420', 'S1459', 'S1461', 'S1470', 'S1471', 'S1472'] # 'S1422', \n",
    "# group_2_sess = all_session_nums\n",
    "# group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "# # Chat Cre control: S1355-1358, S1374, S1376\n",
    "# # group_3_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1374', 'S1376']\n",
    "# group_3_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1374', 'S1376',\n",
    "#                 'S1448', 'S1449', 'S1450', 'S1451']\n",
    "# group_3_sess = all_session_nums\n",
    "# group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "# # Chat Cre control: S1399-1401\n",
    "# group_4_mice = ['S1399', 'S1400', 'S1401']\n",
    "# group_4_sess = all_session_nums\n",
    "# group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "# group_5_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1374', 'S1376',\n",
    "#                'S1448', 'S1449', 'S1450', 'S1451'\n",
    "#                'S1399', 'S1400', 'S1401']\n",
    "# group_5_sess = all_session_nums\n",
    "# group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "# group_6_mice = []\n",
    "# group_6_sess = []\n",
    "# group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# channel_definitions = {}\n",
    "# # channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_1_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_2_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_3_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_4_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "# out_files = {}\n",
    "\n",
    "# fig_5_signal_files_setup = [group_1_combo, group_2_combo, group_3_combo, group_4_combo, group_5_combo, group_6_combo]\n",
    "# for g_num in range(0, len(fig_5_signal_files_setup)):\n",
    "#     fig_5_basis = fig_5_signal_files_setup[g_num]\n",
    "#     for f in fig_5_basis:\n",
    "#         glob_file = glob.glob(f'../../data/raw-new/{figure}/GLM_SIGNALS_{f}*')\n",
    "#         if len(glob_file) != 1:\n",
    "#             print('Missing file!!! ', f)\n",
    "#         else:\n",
    "#             signal_files += glob_file\n",
    "#             out_files[glob_file[0]] = (figure, figure + '/g' + str(g_num+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Figure 4: Tetanus Mice\n",
    "# figure = 'Figure_4'\n",
    "\n",
    "# # all_session_nums = [\n",
    "# #                     '03172022', '03212022', '03232022', '03252022', '03292022', '03312022', '04032022', '04052022',\n",
    "# #                     '04072022', '04082022', '04112022', '04132022', '04152022', '04202022', '04222022', '05022022',\n",
    "# #                     '05102022', '05162022', '05182022', '05222022', '05232022', '05242022', '05252022', '05262022',\n",
    "# #                     '05272022', '05292022', '05302022', '05312022',\n",
    "# #                     ]\n",
    "# all_session_nums = ['08312021', '09022021', '09062021', '09082021', '09102021', '09132021',\n",
    "#                     '09152021', '09172021', '09202021', '10052021', '10072021', '10122021',\n",
    "#                     '10142021', '10192021', '10212021', '10252021', '10272021', '11302021',\n",
    "#                     '12022021', '12032021', '12062021', '12072021', '12082021', '12092021',\n",
    "#                     '12102021', '12132021', '12142021']\n",
    "\n",
    "# # Control\n",
    "# group_1_mice = ['S1233', 'S1234', 'S1260', 'S1246', 'S1248']\n",
    "# group_1_sess = all_session_nums\n",
    "# group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "# # Tetanus\n",
    "# group_2_mice = ['S1194', 'S1195', 'S1214', 'S1258', 'S1259']\n",
    "# group_2_sess = all_session_nums\n",
    "# group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "# group_3_mice = []\n",
    "# group_3_sess = all_session_nums\n",
    "# group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "# group_4_mice = []\n",
    "# group_4_sess = all_session_nums\n",
    "# group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "# group_5_mice = []\n",
    "# group_5_sess = all_session_nums\n",
    "# group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "# group_6_mice = []\n",
    "# group_6_sess = []\n",
    "# group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# # channel_definitions = {}\n",
    "# # channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gDA'} for file_combo in group_1_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gDA'} for file_combo in group_2_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_3_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_4_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "# fig_4_signal_files_setup = []#[group_1_combo, group_2_combo, group_3_combo, group_4_combo, group_5_combo, group_6_combo]\n",
    "# for g_num in range(0, len(fig_4_signal_files_setup)):\n",
    "#     fig_4_basis = fig_4_signal_files_setup[g_num]\n",
    "#     for f in fig_4_basis:\n",
    "#         glob_file = glob.glob(str(Path(f'../../data/raw-new/{figure}/GLM_SIGNALS_{f}*').resolve()))\n",
    "#         if len(glob_file) != 1:\n",
    "#             print('Missing file!!! ', f)\n",
    "#         else:\n",
    "#             signal_files += glob_file\n",
    "#             out_files[glob_file[0]] = (figure, figure + '/g' + str(g_num+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Figure 6: Glutamate Mice\n",
    "# figure = 'Figure_6'\n",
    "\n",
    "# # all_session_nums = [\n",
    "# #                     '03172022', '03212022', '03232022', '03252022', '03292022', '03312022', '04032022', '04052022',\n",
    "# #                     '04072022', '04082022', '04112022', '04132022', '04152022', '04202022', '04222022', '05022022',\n",
    "# #                     '05102022', '05162022', '05182022', '05222022', '05232022', '05242022', '05252022', '05262022',\n",
    "# #                     '05272022', '05292022', '05302022', '05312022',\n",
    "# #                     ]\n",
    "# all_session_nums = [\n",
    "#                     '01312022', '02022022', '02072022'\n",
    "#                    ]\n",
    "\n",
    "# # Glutamate\n",
    "# group_1_mice = ['S1299', 'S1300', 'S1301', 'S1302']\n",
    "# group_1_sess = all_session_nums\n",
    "# group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "# group_2_mice = []\n",
    "# group_2_sess = all_session_nums\n",
    "# group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "# group_3_mice = []\n",
    "# group_3_sess = all_session_nums\n",
    "# group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "# group_4_mice = []\n",
    "# group_4_sess = all_session_nums\n",
    "# group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "# group_5_mice = []\n",
    "# group_5_sess = all_session_nums\n",
    "# group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "# group_6_mice = []\n",
    "# group_6_sess = []\n",
    "# group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# # channel_definitions = {}\n",
    "# # channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gGLUl', 'Ch1':'gGLUr'} for file_combo in group_1_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gGLUl', 'Ch1':'gGLUr'} for file_combo in group_2_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gGLUl', 'Ch1':'gGLUr'} for file_combo in group_3_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_4_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "# fig_6_signal_files_setup = [group_1_combo, group_2_combo, group_3_combo, group_4_combo, group_5_combo, group_6_combo]\n",
    "# for g_num in range(0, len(fig_6_signal_files_setup)):\n",
    "#     fig_6_basis = fig_6_signal_files_setup[g_num]\n",
    "#     for f in fig_6_basis:\n",
    "#         glob_file = glob.glob(str(Path(f'../../data/raw-old/GLM_SIGNALS_{f}*').resolve()))\n",
    "#         if len(glob_file) != 1:\n",
    "#             print('Missing file!!! ', f)\n",
    "#         else:\n",
    "#             signal_files += glob_file\n",
    "#             str_num_sft = '_'.join([str(_) for _ in num_sft])\n",
    "#             out_files[glob_file[0]] = (figure, figure + f'/g{g_num+1}-{str_num_sft}sft')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing file!!!  S732_06152020\n",
      "Missing file!!!  S732_06272020\n",
      "Missing file!!!  S732_06292020\n",
      "Missing file!!!  S732_07012020\n",
      "Missing file!!!  S735_06172020\n",
      "Missing file!!!  S735_06272020\n",
      "Missing file!!!  S735_06292020\n",
      "Missing file!!!  S735_07012020\n",
      "Missing file!!!  S736_06152020\n",
      "Missing file!!!  S736_06272020\n",
      "Missing file!!!  S736_06292020\n",
      "Missing file!!!  S736_07012020\n",
      "Missing file!!!  S776_06152020\n",
      "Missing file!!!  S776_06172020\n",
      "Missing file!!!  S776_06212020\n"
     ]
    }
   ],
   "source": [
    "### Figure 7: Glutamate Mice\n",
    "figure = 'Figure_7'\n",
    "\n",
    "# all_session_nums = [\n",
    "#                     '03172022', '03212022', '03232022', '03252022', '03292022', '03312022', '04032022', '04052022',\n",
    "#                     '04072022', '04082022', '04112022', '04132022', '04152022', '04202022', '04222022', '05022022',\n",
    "#                     '05102022', '05162022', '05182022', '05222022', '05232022', '05242022', '05252022', '05262022',\n",
    "#                     '05272022', '05292022', '05302022', '05312022',\n",
    "#                     ]\n",
    "all_session_nums = [\n",
    "                    '06152020',\n",
    "                    '06172020',\n",
    "                    '06212020',\n",
    "                    '06272020',\n",
    "                    '06292020',\n",
    "                    '07012020',\n",
    "                   ]\n",
    "\n",
    "# Control / Tetatnus Treatment\n",
    "group_1_mice = ['S732', 'S735', 'S736', 'S776',]\n",
    "group_1_sess = all_session_nums\n",
    "group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    " \n",
    "group_2_mice = []\n",
    "group_2_sess = all_session_nums\n",
    "group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "group_3_mice = []\n",
    "group_3_sess = all_session_nums\n",
    "group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "group_4_mice = []\n",
    "group_4_sess = all_session_nums\n",
    "group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "group_5_mice = []\n",
    "group_5_sess = all_session_nums\n",
    "group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "group_6_mice = []\n",
    "group_6_sess = []\n",
    "group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# channel_definitions = {}\n",
    "# channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gDA_ctrl', 'Ch1':'gDA_tet'} for file_combo in group_1_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch1':'gDA'} for file_combo in group_2_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gGLUl', 'Ch1':'gGLUr'} for file_combo in group_3_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_4_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "fig_6_signal_files_setup = [group_1_combo, group_2_combo, group_3_combo, group_4_combo, group_5_combo, group_6_combo]\n",
    "for g_num in range(0, len(fig_6_signal_files_setup)):\n",
    "    fig_6_basis = fig_6_signal_files_setup[g_num]\n",
    "    for f in fig_6_basis:\n",
    "        glob_file = glob.glob(str(Path(f'../../data/raw-old/{figure}/GLM_SIGNALS_{f}*').resolve()))\n",
    "        if len(glob_file) != 1:\n",
    "            print('Missing file!!! ', f)\n",
    "        else:\n",
    "            signal_files += glob_file\n",
    "            str_num_sft = '_'.join([str(_) for _ in num_sft])\n",
    "            out_files[glob_file[0]] = (figure, figure + f'/g{g_num+1}-{str_num_sft}sft')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# figure = 'fig3-dualhem'\n",
    "# print(sorted(list(set([_.split('_')[-1].replace('.txt', '') for _ in glob.glob(f'../../data/raw-new/{figure}/GLM_SIGNALS_*') if 'WT6' in _]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure = 'fig5'\n",
    "# print(sorted(list(set([_.split('_')[-1].replace('.txt', '') for _ in glob.glob(f'../../data/raw-new/{figure}/GLM_SIGNALS_*')]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal_files = ['/Users/josh/Documents/Harvard/GLM/sabatinilab-glm/sglm/data/raw-new/fig5/GLM_SIGNALS_S1449_06022022.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('S732_06152020',)\n",
      "('S732_06172020',)\n",
      "> GLM_SIGNALS_S732_06172020.txt\n",
      "('S732_06212020',)\n",
      "> GLM_SIGNALS_S732_06212020.txt\n",
      "('S732_06272020',)\n",
      "('S732_06292020',)\n",
      "('S732_07012020',)\n",
      "('S735_06152020',)\n",
      "> GLM_SIGNALS_S735_06152020.txt\n",
      "('S735_06172020',)\n",
      "('S735_06212020',)\n",
      "> GLM_SIGNALS_S735_06212020.txt\n",
      "('S735_06272020',)\n",
      "('S735_06292020',)\n",
      "('S735_07012020',)\n",
      "('S736_06152020',)\n",
      "('S736_06172020',)\n",
      "> GLM_SIGNALS_S736_06172020.txt\n",
      "('S736_06212020',)\n",
      "> GLM_SIGNALS_S736_06212020.txt\n",
      "('S736_06272020',)\n",
      "('S736_06292020',)\n",
      "('S736_07012020',)\n",
      "('S776_06152020',)\n",
      "('S776_06172020',)\n",
      "('S776_06212020',)\n",
      "('S776_06272020',)\n",
      "> GLM_SIGNALS_S776_06272020.txt\n",
      "('S776_06292020',)\n",
      "> GLM_SIGNALS_S776_06292020.txt\n",
      "('S776_07012020',)\n",
      "> GLM_SIGNALS_S776_07012020.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a56d0eeba404babb43f030b36de933d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\raw-old\\Figure_7\\GLM_SIGNALS_S732_06172020.txt\n",
      "C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\raw-old\\Figure_7\\GLM_SIGNALS_S732_06212020.txt\n",
      "C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\raw-old\\Figure_7\\GLM_SIGNALS_S735_06152020.txt\n",
      "C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\raw-old\\Figure_7\\GLM_SIGNALS_S735_06212020.txt\n",
      "C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\raw-old\\Figure_7\\GLM_SIGNALS_S736_06172020.txt\n",
      "C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\raw-old\\Figure_7\\GLM_SIGNALS_S736_06212020.txt\n",
      "C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\raw-old\\Figure_7\\GLM_SIGNALS_S776_06272020.txt\n",
      "C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\raw-old\\Figure_7\\GLM_SIGNALS_S776_06292020.txt\n",
      "C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\raw-old\\Figure_7\\GLM_SIGNALS_S776_07012020.txt\n"
     ]
    }
   ],
   "source": [
    "for ign in ignore_files:\n",
    "    signal_files = [_ for _ in signal_files if ign not in _]\n",
    "\n",
    "table_files = [_.replace('GLM_SIGNALS', 'GLM_TABLE') for _ in signal_files]\n",
    "\n",
    "\n",
    "channel_assignments = bf.get_rename_columns_by_file(signal_files, channel_definitions)\n",
    "\n",
    "for file_num in trange(len(signal_files)):\n",
    "\n",
    "    ## Load Table Data\n",
    "    # signal_fn = signal_files[0]\n",
    "    # table_fn = table_files[0]\n",
    "\n",
    "    signal_path = signal_files[file_num]\n",
    "    table_path = table_files[file_num]\n",
    "\n",
    "    signal_fn = Path(signal_files[file_num]).parts[-1]\n",
    "    table_fn = Path(table_files[file_num]).parts[-1]\n",
    "\n",
    "    signal_filename_out = signal_fn.replace('GLM_SIGNALS', 'GLM_SIGNALS_INTERIM').replace('txt', 'csv')\n",
    "    table_filename_out = table_fn.replace('GLM_TABLE', 'GLM_TABLE_INTERIM').replace('txt', 'csv')\n",
    "\n",
    "    # signal_path_out = f'../../data/interim2/{signal_filename_out}'\n",
    "    # table_path_out = f'../../data/interim2/{table_filename_out}'\n",
    "\n",
    "    signal_path_out = signal_path.replace(r'raw', r'interim').replace('GLM_SIGNALS', 'GLM_SIGNALS_INTERIM').replace('txt', 'csv')\n",
    "    table_path_out = table_path.replace(r'raw', r'interim').replace('GLM_TABLE', 'GLM_TABLE_INTERIM').replace('txt', 'csv')\n",
    "\n",
    "    \n",
    "    if out_files is not None and signal_path in out_files:\n",
    "        signal_path_out = signal_path_out.replace(*out_files[signal_path])\n",
    "        table_path_out = table_path_out.replace(*out_files[signal_path])\n",
    "\n",
    "    create_folder_if_not_exists(str(Path('/'.join(Path(signal_path_out).parts[:-1])).resolve()))\n",
    "    create_folder_if_not_exists(str(Path('/'.join(Path(table_path_out).parts[:-1])).resolve()))\n",
    "    \n",
    "    signal_df = pd.read_csv(signal_path)\n",
    "    table_df = pd.read_csv(table_path)\n",
    "\n",
    "    # Check for multiple copies of the same sideIn\n",
    "    eq = table_df['photometrySideInIndex'].dropna()\n",
    "    eq = eq[eq != 0]\n",
    "    if len(eq) != eq.nunique():\n",
    "        print(len(eq))\n",
    "        print(eq.nunique())\n",
    "        display(eq)\n",
    "        print(f'Error: Multiple side ins with the same timestamp detected for {signal_path}. Continuing...')\n",
    "        continue\n",
    "#     signal_df.to_csv(r'C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\out_tmp\\sdf_b_v90.csv')\n",
    "\n",
    "    print(signal_path)\n",
    "    signal_df, table_df = gsd.generate_signal_df(signal_path,\n",
    "                                            table_path,\n",
    "                                            # signal_filename_out=f'../../data/interim2/{signal_filename_out}',\n",
    "                                            # table_filename_out=f'../../data/interim2/{table_filename_out}',\n",
    "#                                             trial_bounds_before_center_in = -50,\n",
    "#                                             trial_bounds_after_side_out = 50,\n",
    "                                                 \n",
    "                                            trial_bounds_before_center_in = num_sft[0],\n",
    "                                            trial_bounds_after_side_out = num_sft[1],\n",
    "                                            )\n",
    "\n",
    "#     signal_df.to_csv(r'C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\out_tmp\\sdf_b_v00.csv')\n",
    "    signal_df = signal_df[signal_df['nTrial'] > 0].fillna(0)\n",
    "#     signal_df.to_csv(r'C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\data\\out_tmp\\sdf_b_v01.csv')\n",
    "\n",
    "    # Break down Preprocess Lynne into component parts\n",
    "\n",
    "    # Rename Columns\n",
    "    signal_df = bf.rename_consistent_columns(signal_df)\n",
    "    \n",
    "    # print(channel_assignments.keys())\n",
    "    # print(signal_fn)\n",
    "    if signal_fn in channel_assignments:\n",
    "        signal_df = signal_df.rename(channel_assignments[signal_fn], axis=1)\n",
    "\n",
    "    for y_col in y_col_lst_all:\n",
    "        if y_col not in signal_df.columns:\n",
    "            signal_df[y_col] = np.nan\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    ## Set Full Trial Reward Flags\n",
    "    signal_df['r_trial'] = (signal_df.groupby('nTrial')['photometrySideInIndexr'].transform(np.sum) > 0) * 1.0\n",
    "    signal_df['nr_trial'] = (signal_df.groupby('nTrial')['photometrySideInIndexnr'].transform(np.sum) > 0) * 1.0\n",
    "\n",
    "    ## Define Side Rewarded / Unrewarded Flags\n",
    "    signal_df = bf.set_port_entry_exit_rewarded_unrewarded_indicators(signal_df)\n",
    "    \n",
    "    \n",
    "    # TODO: FIX ONE-OFF\n",
    "    signal_df = signal_df.rename({'rightLick':'rl', 'leftLick':'ll'},axis=1)\n",
    "    \n",
    "    ## Define Side Agnostic Events\n",
    "    signal_df = bf.define_side_agnostic_events(signal_df)\n",
    "\n",
    "    # print('Percent of Data in ITI:', (df['nTrial'] == df['nEndTrial']).mean())\n",
    "\n",
    "    signal_df['spnrOff'] = ((signal_df['spnr'] == 1)&(signal_df['photometrySideInIndex'] != 1)).astype(int)\n",
    "    signal_df['spxrOff'] = ((signal_df['spxr'] == 1)&(signal_df['photometrySideOutIndex'] != 1)).astype(int)\n",
    "    spnnrOff_a = ((signal_df['spnnr'] == 1)&(signal_df['photometrySideInIndex'] != 1)).astype(int)\n",
    "    spxnrOff_a = ((signal_df['spxnr'] == 1)&(signal_df['photometrySideOutIndex'] != 1)).astype(int)\n",
    "\n",
    "    # If we have something listed as a rewarded \"off\" side entry labeled in the table as a side exit... it means it was a fast \"out-in\".\n",
    "    # The latter \"in\" should be considered an unrewarded side port \"off\" entry.\n",
    "    dualism_exen = ((signal_df['spnrOff'] == 1)&(signal_df['photometrySideOutIndex'] == 1)).astype(int)\n",
    "\n",
    "    # Unrewarded side port entries should be the combination of those simply identified by checking spnnr & the table labels +\n",
    "    # the dualism defined immediately prior. Then those dualism examples should be remoed from the \"off\" rewarded entries.\n",
    "    signal_df['spnnrOff'] = spnnrOff_a + dualism_exen\n",
    "    signal_df['spnrOff'] = signal_df['spnrOff'] - dualism_exen\n",
    "\n",
    "    signal_df['spxnrOff'] = spxnrOff_a\n",
    "\n",
    "    \n",
    "\n",
    "    signal_df['slOff'] = signal_df['sl'] * signal_df['nr_trial']\n",
    "    signal_df['slOn'] = signal_df['sl'] - signal_df['slOff']\n",
    "\n",
    "\n",
    "    \n",
    "    signal_df['cpnOff'] = ((signal_df['cpn'] == 1)&(signal_df['photometryCenterInIndex'] != 1)).astype(int)\n",
    "    signal_df['cpxOff'] = ((signal_df['cpx'] == 1)&(signal_df['photometryCenterOutIndex'] != 1)).astype(int)\n",
    "    # spnnrOff_a = ((signal_df['cpnOff'] == 1)&(signal_df['photometryCenterInIndex'] != 1)).astype(int)\n",
    "    # spxnrOff_a = ((signal_df['cpxOff'] == 1)&(signal_df['photometryCenterOutIndex'] != 1)).astype(int)\n",
    "\n",
    "    # # If we have something listed as a rewarded \"off\" side entry labeled in the table as a side exit... it means it was a fast \"out-in\".\n",
    "    # # The latter \"in\" should be considered an unrewarded side port \"off\" entry.\n",
    "    # dualism_exen = ((signal_df['cpnOff'] == 1)&(signal_df['photometryCenterOutIndex'] == 1)).astype(int)\n",
    "\n",
    "    # # Unrewarded side port entries should be the combination of those simply identified by checking spnnr & the table labels +\n",
    "    # # the dualism defined immediately prior. Then those dualism examples should be remoed from the \"off\" rewarded entries.\n",
    "    # signal_df['spnnrOff'] = spnnrOff_a + dualism_exen\n",
    "    # signal_df['spnrOff'] = signal_df['spnrOff'] - dualism_exen\n",
    "\n",
    "    # signal_df['spxnrOff'] = spxnrOff_a\n",
    "\n",
    "    if signal_path_out:\n",
    "        signal_df.to_csv(signal_path_out, index_label='index')\n",
    "    if table_path_out:\n",
    "        table_df.to_csv(table_path_out, index_label='index')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa0fc083a9a7b25dab36cbe71fb89b2f1907d4eced1698b208dea6977346b521"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
