{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sglm.models import sglm\n",
    "from sglm.features import gen_signal_df as gsd\n",
    "from sglm.features import build_features as bf\n",
    "from sglm.features import gen_signal_df as gsd\n",
    "from sglm.features import build_features as bf\n",
    "\n",
    "import itertools\n",
    "\n",
    "signal_files = []\n",
    "out_files = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_col_lst_all = ['gACH', 'rDA', 'gDA',              \n",
    "                 'gACH_flx_drd', 'rDA_flx_drd', 'gDA_flx_drd',\n",
    "                 'gACH_flx_cht', 'rDA_flx_cht', 'gDA_flx_cht',\n",
    "                 'gACH_flx_drdcht', 'rDA_flx_drdcht', 'gDA_flx_drdcht',\n",
    "                 'Ch5', 'Ch6', 'GP_1', 'GP_2', 'GP_5', 'GP_6', 'SGP_1', 'SGP_2', 'SGP_5', 'SGP_6']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def create_folder_if_not_exists(dir):\n",
    "    dir = str(Path(dir).resolve())\n",
    "    constructed_dir = '/'\n",
    "    for fold in dir.split('/'):\n",
    "        if len(fold) == 0:\n",
    "            continue\n",
    "        constructed_dir += fold + '/'\n",
    "        if os.path.isdir(constructed_dir):\n",
    "            print(f'Directory already exists:', constructed_dir)\n",
    "        else:\n",
    "            print(f'Creating directory:', constructed_dir)\n",
    "            os.mkdir(constructed_dir)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Figure 1: Single side recording\n",
    "figure = 'fig1'\n",
    "\n",
    "# # Load Signal Data\n",
    "\n",
    "# signal_files = glob.glob(f'../../data/raw/GLM_SIGNALS_WT61_*')\n",
    "# signal_files += glob.glob(f'../../data/raw/GLM_SIGNALS_WT63_*')\n",
    "# signal_files += glob.glob(f'../../data/raw/GLM_SIGNALS_WT64_*')\n",
    "\n",
    "ignore_files = [\n",
    "                'WT61_10152021',\n",
    "                'WT61_10082021'\n",
    "                ]\n",
    "# for ign in ignore_files:\n",
    "#     signal_files = [_ for _ in signal_files if ign not in _]\n",
    "\n",
    "# table_files = [_.replace('GLM_SIGNALS', 'GLM_TABLE') for _ in signal_files]\n",
    "\n",
    "# channel_definitions = {\n",
    "#         ('WT61',): {'Ch1': 'gACH', 'Ch2': 'rDA'},\n",
    "#         ('WT64',): {'Ch1': 'gACH', 'Ch2': 'empty'},\n",
    "#         ('WT63',): {'Ch1': 'gDA', 'Ch2': 'empty'},\n",
    "#     }\n",
    "\n",
    "group_1_mice = ['WT63', 'WT64', 'WT65']\n",
    "# group_1_sess = ['11082021', '11102021', '11122021', '11182021']\n",
    "group_1_sess = ['11082021', '11102021', '11122021', '11162021', '11182021']\n",
    "group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "group_2_mice = ['WT66', 'WT67', 'WT68', 'WT69']\n",
    "group_2_sess = ['12132021', '12152021', '12172021', '12192021']\n",
    "group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "group_3_mice = ['WT58', 'WT60', 'WT61']\n",
    "group_3_sess = ['10042021', '10062021', '10082021', '10112021', '10132021', '10152021']\n",
    "group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "group_4_mice = ['WT53', 'WT54', 'WT55', 'WT56']\n",
    "group_4_sess = ['09012021', '09032021', '09062021']\n",
    "group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "group_5_mice = ['WT57', 'WT59']\n",
    "group_5_sess = ['10042021', '10062021', '10082021', '10112021', '10132021', '10152021']\n",
    "group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "group_6_mice = []#['WT62']\n",
    "group_6_sess = []#['11082021', '11102021', '11122021', '11182021']\n",
    "group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "channel_definitions = {}\n",
    "# channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "channel_definitions.update({(file_combo,): {'Ch1': 'gDA', 'Ch5': 'gACH'} for file_combo in group_1_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch1': 'gDA'} for file_combo in group_2_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch1': 'gACH'} for file_combo in group_3_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch1': 'gACH'} for file_combo in group_4_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH'} for file_combo in group_5_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch1': 'gDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig_1_signal_files_setup = group_1_combo + group_2_combo + group_3_combo + group_4_combo + group_5_combo + group_6_combo\n",
    "\n",
    "for f in fig_1_signal_files_setup:\n",
    "    glob_file = glob.glob(f'../../data/raw/{figure}/GLM_SIGNALS_{f}*')\n",
    "    if len(glob_file) != 1:\n",
    "        print('Missing file!!! ', f)\n",
    "    signal_files += glob_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure 3: Dual side recording\n",
    "figure = 'fig3'\n",
    "\n",
    "# 'WT61', 'WT63', 'WT64', '', 'WT44', 'WT51' # Excluding 'WT43\n",
    "\n",
    "# ['03162021','03192021','03232021','03262021','07282021','07302021','08012021', '08042021', '08062021', '08102021', '10042021',\n",
    "# '10082021', '10112021', '10132021', '10182021', '11082021', '11102021', '11122021', '11162021', '11182021']\n",
    "\n",
    "group_1_mice = ['WT63', 'WT64', 'WT44', 'WT51']\n",
    "group_1_sess = ['03162021','03192021','03232021','03262021','07282021','07302021','08012021', '08042021', '08062021', '08102021', '10042021',\n",
    "                '10082021', '10112021', '10132021', '10182021', '11082021', '11102021', '11122021', '11162021', '11182021']\n",
    "group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "group_2_mice = ['WT61']\n",
    "group_2_sess = ['03162021','03192021','03232021','03262021','07282021','07302021','08012021', '08042021', '08062021', '08102021', '10042021',\n",
    "                '10082021', '10112021', '10132021', '11082021', '11102021', '11122021', '11162021', '11182021']\n",
    "group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "# group_3_mice = ['WT58', 'WT60', 'WT61']\n",
    "# group_3_sess = ['10042021', '10062021', '10082021', '10112021', '10132021', '10152021']\n",
    "# group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "# group_4_mice = ['WT53', 'WT54', 'WT55', 'WT56']\n",
    "# group_4_sess = ['09012021', '09032021', '09062021']\n",
    "# group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "# group_5_mice = ['WT61']\n",
    "# group_5_sess = ['10042021', '10062021', '10082021', '10112021', '10132021', '10152021']\n",
    "# group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "group_6_mice = ['WT43']\n",
    "group_6_sess = []\n",
    "group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# channel_definitions = {}\n",
    "# channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_1_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_2_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch1': 'gACH'} for file_combo in group_3_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch1': 'gACH'} for file_combo in group_4_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "fig_3_signal_files_setup = group_1_combo + group_2_combo #+ group_3_combo + group_4_combo + group_5_combo + group_6_combo\n",
    "for f in fig_3_signal_files_setup:\n",
    "    glob_file = glob.glob(f'../../data/raw/{figure}/GLM_SIGNALS_{f}*')\n",
    "    if len(glob_file) != 1:\n",
    "        print('Missing file!!! ', f)\n",
    "    signal_files += glob_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure 3 dualhem\n",
    "figure = 'fig3-dualhem'\n",
    "\n",
    "group_1_mice = ['WT62', 'WT63', 'WT64', 'WT65']\n",
    "group_1_sess = ['10042021', '10082021', '10112021', '10132021',\n",
    "                '10182021', '11082021', '11102021', '11122021', '11162021', '11182021']\n",
    "group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "group_2_mice = []\n",
    "group_2_sess = []\n",
    "group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "group_3_mice = []\n",
    "group_3_sess = []\n",
    "group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "group_4_mice = []\n",
    "group_4_sess = []\n",
    "group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "group_5_mice = []\n",
    "group_5_sess = []\n",
    "group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "group_6_mice = []\n",
    "group_6_sess = []\n",
    "group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# channel_definitions = {}\n",
    "# channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_1_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_2_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_3_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_4_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "fig_3dh_signal_files_setup = [group_1_combo, group_2_combo, group_3_combo, group_4_combo, group_5_combo, group_6_combo]\n",
    "for g_num in range(0, len(fig_3dh_signal_files_setup)):\n",
    "    fig_3dh_basis = fig_3dh_signal_files_setup[g_num]\n",
    "    for f in fig_3dh_basis:\n",
    "        glob_file = glob.glob(f'../../data/raw/{figure}/GLM_SIGNALS_{f}*')\n",
    "        if len(glob_file) != 1:\n",
    "            print('Missing file!!! ', f)\n",
    "        else:\n",
    "            signal_files += glob_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure 5: Flox Mice\n",
    "figure = 'fig5'\n",
    "\n",
    "# all_session_nums = [\n",
    "#                     '03172022', '03212022', '03232022', '03252022', '03292022', '03312022', '04032022', '04052022',\n",
    "#                     '04072022', '04082022', '04112022', '04132022', '04152022', '04202022', '04222022', '05022022',\n",
    "#                     '05102022', '05162022', '05182022', '05222022', '05232022', '05242022', '05252022', '05262022',\n",
    "#                     '05272022', '05292022', '05302022', '05312022',\n",
    "#                     ]\n",
    "all_session_nums = [\n",
    "                    '03172022', '03212022', '03232022', '03252022', '03282022',\n",
    "                    '03292022', '03302022', '03312022', '04012022', '04032022',\n",
    "                    '04052022', '04072022', '04082022', '04112022', '04132022',\n",
    "                    '04152022', '04202022', '04222022', '05022022', '05102022',\n",
    "                    '05162022', '05182022', '05222022', '05232022', '05242022',\n",
    "                    '05252022', '05262022', '05272022', '05292022', '05302022',\n",
    "                    '05312022', '06022022', '06062022', '06082022', '06102022',\n",
    "                    '06132022', '06152022', '06182022', '07072022', '07082022',\n",
    "                    '07092022', '07112022', '07122022', '07132022', '07142022',\n",
    "                    '07152022', '07162022', '07172022', '07182022', '07192022'\n",
    "                   ]\n",
    "\n",
    "# Drd2f/f control: S1417, 1419, 1421\n",
    "# group_1_mice = ['S1417', 'S1419', 'S1421']\n",
    "group_1_mice = ['S1417', 'S1419', 'S1421', 'S1460', 'S1462', 'S1473', 'S1474']\n",
    "group_1_sess = all_session_nums\n",
    "group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "# Chat Cre X Drd2f/f: S1416, 1418, 1420, 1422\n",
    "# group_2_mice = ['S1416', 'S1418', 'S1420', 'S1422']\n",
    "group_2_mice = ['S1416', 'S1418', 'S1420', 'S1459', 'S1461', 'S1470', 'S1471', 'S1472'] # 'S1422', \n",
    "group_2_sess = all_session_nums\n",
    "group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "# Chat Cre control: S1355-1358, S1376\n",
    "# group_3_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1374', 'S1376']\n",
    "group_3_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1376',\n",
    "                'S1448', 'S1449', 'S1450', 'S1451']\n",
    "group_3_sess = all_session_nums\n",
    "group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "# Chat Cre control: S1399-1401\n",
    "group_4_mice = ['S1399', 'S1400', 'S1401']\n",
    "group_4_sess = all_session_nums\n",
    "group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "# group_5_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1374', 'S1376',\n",
    "#                'S1448', 'S1449', 'S1450', 'S1451'\n",
    "#                'S1399', 'S1400', 'S1401']\n",
    "group_5_mice = []\n",
    "group_5_sess = all_session_nums\n",
    "group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "group_6_mice = []\n",
    "group_6_sess = []\n",
    "group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# channel_definitions = {}\n",
    "# channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_1_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_2_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_3_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_4_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "out_files = {}\n",
    "\n",
    "fig_5_signal_files_setup = [group_1_combo, group_2_combo, group_3_combo, group_4_combo, group_5_combo, group_6_combo]\n",
    "for g_num in range(0, len(fig_5_signal_files_setup)):\n",
    "    fig_5_basis = fig_5_signal_files_setup[g_num]\n",
    "    for f in fig_5_basis:\n",
    "        glob_file = glob.glob(f'../../data/raw/{figure}/GLM_SIGNALS_{f}*')\n",
    "        if len(glob_file) != 1:\n",
    "            print('Missing file!!! ', f)\n",
    "        else:\n",
    "            signal_files += glob_file\n",
    "            out_files[glob_file[0]] = (figure, figure + '/g' + str(g_num+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Figure 5: Flox Mice\n",
    "# figure = 'fig5'\n",
    "\n",
    "# # all_session_nums = [\n",
    "# #                     '03172022', '03212022', '03232022', '03252022', '03292022', '03312022', '04032022', '04052022',\n",
    "# #                     '04072022', '04082022', '04112022', '04132022', '04152022', '04202022', '04222022', '05022022',\n",
    "# #                     '05102022', '05162022', '05182022', '05222022', '05232022', '05242022', '05252022', '05262022',\n",
    "# #                     '05272022', '05292022', '05302022', '05312022',\n",
    "# #                     ]\n",
    "# all_session_nums = [\n",
    "#                     '03172022', '03212022', '03232022', '03252022', '03282022',\n",
    "#                     '03292022', '03302022', '03312022', '04012022', '04032022',\n",
    "#                     '04052022', '04072022', '04082022', '04112022', '04132022',\n",
    "#                     '04152022', '04202022', '04222022', '05022022', '05102022',\n",
    "#                     '05162022', '05182022', '05222022', '05232022', '05242022',\n",
    "#                     '05252022', '05262022', '05272022', '05292022', '05302022',\n",
    "#                     '05312022', '06022022', '06062022', '06082022', '06102022',\n",
    "#                     '06132022', '06152022', '06182022', '07072022', '07082022',\n",
    "#                     '07092022', '07112022', '07122022', '07132022', '07142022',\n",
    "#                     '07152022', '07162022', '07172022', '07182022', '07192022'\n",
    "#                    ]\n",
    "\n",
    "# # Drd2f/f control: S1417, 1419, 1421\n",
    "# # group_1_mice = ['S1417', 'S1419', 'S1421']\n",
    "# group_1_mice = ['S1417', 'S1419', 'S1421', 'S1460', 'S1462', 'S1473', 'S1474']\n",
    "# group_1_sess = all_session_nums\n",
    "# group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "# # Chat Cre X Drd2f/f: S1416, 1418, 1420, 1422\n",
    "# # group_2_mice = ['S1416', 'S1418', 'S1420', 'S1422']\n",
    "# group_2_mice = ['S1416', 'S1418', 'S1420', 'S1459', 'S1461', 'S1470', 'S1471', 'S1472'] # 'S1422', \n",
    "# group_2_sess = all_session_nums\n",
    "# group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "# # Chat Cre control: S1355-1358, S1374, S1376\n",
    "# # group_3_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1374', 'S1376']\n",
    "# group_3_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1374', 'S1376',\n",
    "#                 'S1448', 'S1449', 'S1450', 'S1451']\n",
    "# group_3_sess = all_session_nums\n",
    "# group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "# # Chat Cre control: S1399-1401\n",
    "# group_4_mice = ['S1399', 'S1400', 'S1401']\n",
    "# group_4_sess = all_session_nums\n",
    "# group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "# group_5_mice = ['S1355', 'S1356', 'S1357', 'S1358', 'S1374', 'S1376',\n",
    "#                'S1448', 'S1449', 'S1450', 'S1451'\n",
    "#                'S1399', 'S1400', 'S1401']\n",
    "# group_5_sess = all_session_nums\n",
    "# group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "# group_6_mice = []\n",
    "# group_6_sess = []\n",
    "# group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# channel_definitions = {}\n",
    "# # channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_1_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_2_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_3_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_4_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "# # channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "# out_files = {}\n",
    "\n",
    "# fig_5_signal_files_setup = [group_1_combo, group_2_combo, group_3_combo, group_4_combo, group_5_combo, group_6_combo]\n",
    "# for g_num in range(0, len(fig_5_signal_files_setup)):\n",
    "#     fig_5_basis = fig_5_signal_files_setup[g_num]\n",
    "#     for f in fig_5_basis:\n",
    "#         glob_file = glob.glob(f'../../data/raw/{figure}/GLM_SIGNALS_{f}*')\n",
    "#         if len(glob_file) != 1:\n",
    "#             print('Missing file!!! ', f)\n",
    "#         else:\n",
    "#             signal_files += glob_file\n",
    "#             out_files[glob_file[0]] = (figure, figure + '/g' + str(g_num+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure 4: Tetanus Mice\n",
    "figure = 'fig4'\n",
    "\n",
    "# all_session_nums = [\n",
    "#                     '03172022', '03212022', '03232022', '03252022', '03292022', '03312022', '04032022', '04052022',\n",
    "#                     '04072022', '04082022', '04112022', '04132022', '04152022', '04202022', '04222022', '05022022',\n",
    "#                     '05102022', '05162022', '05182022', '05222022', '05232022', '05242022', '05252022', '05262022',\n",
    "#                     '05272022', '05292022', '05302022', '05312022',\n",
    "#                     ]\n",
    "all_session_nums = ['08312021', '09022021', '09062021', '09082021', '09102021', '09132021',\n",
    "                    '09152021', '09172021', '09202021', '10052021', '10072021', '10122021',\n",
    "                    '10142021', '10192021', '10212021', '10252021', '10272021', '11302021',\n",
    "                    '12022021', '12032021', '12062021', '12072021', '12082021', '12092021',\n",
    "                    '12102021', '12132021', '12142021']\n",
    "\n",
    "# Control\n",
    "group_1_mice = ['S1233', 'S1234', 'S1260', 'S1246', 'S1248']\n",
    "group_1_sess = all_session_nums\n",
    "group_1_combo = ['_'.join(_) for _ in list(itertools.product(group_1_mice, group_1_sess))]\n",
    "\n",
    "# Tetanus\n",
    "group_2_mice = ['S1194', 'S1195', 'S1214', 'S1258', 'S1259']\n",
    "group_2_sess = all_session_nums\n",
    "group_2_combo = ['_'.join(_) for _ in list(itertools.product(group_2_mice, group_2_sess))]\n",
    "\n",
    "group_3_mice = []\n",
    "group_3_sess = all_session_nums\n",
    "group_3_combo = ['_'.join(_) for _ in list(itertools.product(group_3_mice, group_3_sess))]\n",
    "\n",
    "group_4_mice = []\n",
    "group_4_sess = all_session_nums\n",
    "group_4_combo = ['_'.join(_) for _ in list(itertools.product(group_4_mice, group_4_sess))]\n",
    "\n",
    "group_5_mice = []\n",
    "group_5_sess = all_session_nums\n",
    "group_5_combo = ['_'.join(_) for _ in list(itertools.product(group_5_mice, group_5_sess))]\n",
    "\n",
    "group_6_mice = []\n",
    "group_6_sess = []\n",
    "group_6_combo = ['_'.join(_) for _ in list(itertools.product(group_6_mice, group_6_sess))]\n",
    "\n",
    "\n",
    "# channel_definitions = {}\n",
    "# channel_definitions = {(file_combo,): {'Ch1': 'gACH', 'Ch2': 'rDA'} for file_combo in group_1_combo}\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gDA'} for file_combo in group_1_combo})\n",
    "channel_definitions.update({(file_combo,): {'Ch5': 'gDA'} for file_combo in group_2_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA', 'Ch1':'gDA'} for file_combo in group_3_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_4_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_5_combo})\n",
    "# channel_definitions.update({(file_combo,): {'Ch5': 'gACH', 'Ch6': 'rDA'} for file_combo in group_6_combo})\n",
    "\n",
    "\n",
    "fig_4_signal_files_setup = [group_1_combo, group_2_combo, group_3_combo, group_4_combo, group_5_combo, group_6_combo]\n",
    "for g_num in range(0, len(fig_4_signal_files_setup)):\n",
    "    fig_4_basis = fig_4_signal_files_setup[g_num]\n",
    "    for f in fig_4_basis:\n",
    "        glob_file = glob.glob(f'../../data/raw/{figure}/GLM_SIGNALS_{f}*')\n",
    "        if len(glob_file) != 1:\n",
    "            print('Missing file!!! ', f)\n",
    "        else:\n",
    "            signal_files += glob_file\n",
    "            out_files[glob_file[0]] = (figure, figure + '/g' + str(g_num+1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# figure = 'fig3-dualhem'\n",
    "# print(sorted(list(set([_.split('_')[-1].replace('.txt', '') for _ in glob.glob(f'../../data/raw/{figure}/GLM_SIGNALS_*') if 'WT6' in _]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure = 'fig5'\n",
    "# print(sorted(list(set([_.split('_')[-1].replace('.txt', '') for _ in glob.glob(f'../../data/raw/{figure}/GLM_SIGNALS_*')]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal_files = ['/Users/josh/Documents/Harvard/GLM/sabatinilab-glm/sglm/data/raw/fig5/GLM_SIGNALS_S1449_06022022.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ign in ignore_files:\n",
    "    signal_files = [_ for _ in signal_files if ign not in _]\n",
    "\n",
    "table_files = [_.replace('GLM_SIGNALS', 'GLM_TABLE') for _ in signal_files]\n",
    "\n",
    "\n",
    "channel_assignments = bf.get_rename_columns_by_file(signal_files, channel_definitions)\n",
    "\n",
    "for file_num in trange(len(signal_files)):\n",
    "\n",
    "    ## Load Table Data\n",
    "    # signal_fn = signal_files[0]\n",
    "    # table_fn = table_files[0]\n",
    "\n",
    "    signal_path = signal_files[file_num]\n",
    "    table_path = table_files[file_num]\n",
    "\n",
    "    signal_fn = signal_files[file_num].split('/')[-1]\n",
    "    table_fn = table_files[file_num].split('/')[-1]\n",
    "\n",
    "    signal_filename_out = signal_fn.replace('GLM_SIGNALS', 'GLM_SIGNALS_INTERIM').replace('txt', 'csv')\n",
    "    table_filename_out = table_fn.replace('GLM_TABLE', 'GLM_TABLE_INTERIM').replace('txt', 'csv')\n",
    "\n",
    "    # signal_path_out = f'../../data/interim/{signal_filename_out}'\n",
    "    # table_path_out = f'../../data/interim/{table_filename_out}'\n",
    "\n",
    "    signal_path_out = signal_path.replace(r'/raw/', r'/interim/').replace('GLM_SIGNALS', 'GLM_SIGNALS_INTERIM').replace('txt', 'csv')\n",
    "    table_path_out = table_path.replace(r'/raw/', r'/interim/').replace('GLM_SIGNALS', 'GLM_SIGNALS_INTERIM').replace('txt', 'csv')\n",
    "\n",
    "    \n",
    "    if out_files is not None and signal_path in out_files:\n",
    "        signal_path_out = signal_path_out.replace(*out_files[signal_path])\n",
    "        table_path_out = table_path_out.replace(*out_files[signal_path])\n",
    "\n",
    "    create_folder_if_not_exists('/'.join(signal_path_out.split('/')[:-1]))\n",
    "    \n",
    "    signal_df = pd.read_csv(signal_path)\n",
    "    table_df = pd.read_csv(table_path)\n",
    "\n",
    "    # Check for multiple copies of the same sideIn\n",
    "    eq = table_df['photometrySideInIndex'].dropna()\n",
    "    eq = eq[eq != 0]\n",
    "    if len(eq) != eq.nunique():\n",
    "        print(len(eq))\n",
    "        print(eq.nunique())\n",
    "        display(eq)\n",
    "        print(f'Error: Multiple side ins with the same timestamp detected for {signal_path}. Continuing...')\n",
    "        continue\n",
    "\n",
    "    print(signal_path)\n",
    "    signal_df, table_df = gsd.generate_signal_df(signal_path,\n",
    "                                            table_path,\n",
    "                                            # signal_filename_out=f'../../data/interim/{signal_filename_out}',\n",
    "                                            # table_filename_out=f'../../data/interim/{table_filename_out}',\n",
    "                                            trial_bounds_before_center_in = -20,\n",
    "                                            trial_bounds_after_side_out = 20,\n",
    "                                            )\n",
    "\n",
    "    signal_df = signal_df[signal_df['nTrial'] > 0].fillna(0)\n",
    "\n",
    "    # Break down Preprocess Lynne into component parts\n",
    "\n",
    "    # Rename Columns\n",
    "    signal_df = bf.rename_consistent_columns(signal_df)\n",
    "\n",
    "    # print(channel_assignments.keys())\n",
    "    # print(signal_fn)\n",
    "    if signal_fn in channel_assignments:\n",
    "        signal_df = signal_df.rename(channel_assignments[signal_fn], axis=1)\n",
    "\n",
    "\n",
    "    for y_col in y_col_lst_all:\n",
    "        if y_col not in signal_df.columns:\n",
    "            signal_df[y_col] = np.nan\n",
    "            continue\n",
    "    \n",
    "    ## Set Full Trial Reward Flags\n",
    "    signal_df['r_trial'] = (signal_df.groupby('nTrial')['photometrySideInIndexr'].transform(np.sum) > 0) * 1.0\n",
    "    signal_df['nr_trial'] = (signal_df.groupby('nTrial')['photometrySideInIndexnr'].transform(np.sum) > 0) * 1.0\n",
    "\n",
    "    ## Define Side Rewarded / Unrewarded Flags\n",
    "    signal_df = bf.set_port_entry_exit_rewarded_unrewarded_indicators(signal_df)\n",
    "\n",
    "    ## Define Side Agnostic Events\n",
    "    signal_df = bf.define_side_agnostic_events(signal_df)\n",
    "\n",
    "    # print('Percent of Data in ITI:', (df['nTrial'] == df['nEndTrial']).mean())\n",
    "\n",
    "    signal_df['spnrOff'] = ((signal_df['spnr'] == 1)&(signal_df['photometrySideInIndex'] != 1)).astype(int)\n",
    "    signal_df['spxrOff'] = ((signal_df['spxr'] == 1)&(signal_df['photometrySideOutIndex'] != 1)).astype(int)\n",
    "    spnnrOff_a = ((signal_df['spnnr'] == 1)&(signal_df['photometrySideInIndex'] != 1)).astype(int)\n",
    "    spxnrOff_a = ((signal_df['spxnr'] == 1)&(signal_df['photometrySideOutIndex'] != 1)).astype(int)\n",
    "\n",
    "    # If we have something listed as a rewarded \"off\" side entry labeled in the table as a side exit... it means it was a fast \"out-in\".\n",
    "    # The latter \"in\" should be considered an unrewarded side port \"off\" entry.\n",
    "    dualism_exen = ((signal_df['spnrOff'] == 1)&(signal_df['photometrySideOutIndex'] == 1)).astype(int)\n",
    "\n",
    "    # Unrewarded side port entries should be the combination of those simply identified by checking spnnr & the table labels +\n",
    "    # the dualism defined immediately prior. Then those dualism examples should be remoed from the \"off\" rewarded entries.\n",
    "    signal_df['spnnrOff'] = spnnrOff_a + dualism_exen\n",
    "    signal_df['spnrOff'] = signal_df['spnrOff'] - dualism_exen\n",
    "\n",
    "    signal_df['spxnrOff'] = spxnrOff_a\n",
    "\n",
    "    \n",
    "\n",
    "    signal_df['slOff'] = signal_df['sl'] * signal_df['nr_trial']\n",
    "    signal_df['slOn'] = signal_df['sl'] - signal_df['slOff']\n",
    "\n",
    "\n",
    "    \n",
    "    signal_df['cpnOff'] = ((signal_df['cpn'] == 1)&(signal_df['photometryCenterInIndex'] != 1)).astype(int)\n",
    "    signal_df['cpxOff'] = ((signal_df['cpx'] == 1)&(signal_df['photometryCenterOutIndex'] != 1)).astype(int)\n",
    "    # spnnrOff_a = ((signal_df['cpnOff'] == 1)&(signal_df['photometryCenterInIndex'] != 1)).astype(int)\n",
    "    # spxnrOff_a = ((signal_df['cpxOff'] == 1)&(signal_df['photometryCenterOutIndex'] != 1)).astype(int)\n",
    "\n",
    "    # # If we have something listed as a rewarded \"off\" side entry labeled in the table as a side exit... it means it was a fast \"out-in\".\n",
    "    # # The latter \"in\" should be considered an unrewarded side port \"off\" entry.\n",
    "    # dualism_exen = ((signal_df['cpnOff'] == 1)&(signal_df['photometryCenterOutIndex'] == 1)).astype(int)\n",
    "\n",
    "    # # Unrewarded side port entries should be the combination of those simply identified by checking spnnr & the table labels +\n",
    "    # # the dualism defined immediately prior. Then those dualism examples should be remoed from the \"off\" rewarded entries.\n",
    "    # signal_df['spnnrOff'] = spnnrOff_a + dualism_exen\n",
    "    # signal_df['spnrOff'] = signal_df['spnrOff'] - dualism_exen\n",
    "\n",
    "    # signal_df['spxnrOff'] = spxnrOff_a\n",
    "\n",
    "    if signal_path_out:\n",
    "        signal_df.to_csv(signal_path_out, index_label='index')\n",
    "    if table_path_out:\n",
    "        table_df.to_csv(table_path_out, index_label='index')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_df[['sl', 'r_trial', 'nr_trial', 'slOff', ]].iloc[1000:2000].plot()\n",
    "signal_df[signal_df['r_trial'] == signal_df['nr_trial']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = signal_df.groupby('nTrial')['slOff'].sum()\n",
    "# a[a > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df['spnnr'] = ((df['spnnr'] == 1)&(df['photometrySideInIndex'] != 1)).astype(int)\n",
    "# # df['spxnr'] = ((df['spxnr'] == 1)&(df['photometrySideOutIndex'] != 1)).astype(int)\n",
    "\n",
    "# X_cols = [_ for _ in X_cols_all if _ not in left_out]\n",
    "\n",
    "# if len(leave_one_out_list) > 1:\n",
    "#     run_id = f'{prefix}_{fn}_{y_col}_drop={\"_\".join(left_out)}'\n",
    "# else:\n",
    "#     run_id = f'{prefix}_{fn}_{y_col}'\n",
    "\n",
    "# dfrel = df.copy()\n",
    "\n",
    "# dfrel, X_cols_sftd = lpp.timeshift_vals(dfrel, X_cols, neg_order=neg_order, pos_order=pos_order)\n",
    "\n",
    "# dfrel_setup, dfrel_holdout = holdout_splits(dfrel,\n",
    "#                                             id_cols=['nTrial'],\n",
    "#                                             perc_holdout=pholdout)\n",
    "# dfrel_setup, dfrel_holdout = dfrel_setup.copy(), dfrel_holdout.copy()\n",
    "\n",
    "# kfold_cv_idx = sglm_ez.cv_idx_by_trial_id(dfrel_setup,\n",
    "#                                           trial_id_columns=['nTrial'],\n",
    "#                                           num_folds=folds,\n",
    "#                                           test_size=pgss)\n",
    "\n",
    "# prediction_X_cols = [_ for _ in X_cols if _ not in ['nTrial']]\n",
    "# prediction_X_cols_sftd = [_ for _ in X_cols_sftd if _ not in ['nTrial']]\n",
    "\n",
    "# X_setup = get_x(dfrel_setup, prediction_X_cols_sftd, keep_rows=None)\n",
    "# y_setup = get_y(dfrel_setup, y_col, keep_rows=None)\n",
    "# X_setup_noiti = get_x(dfrel_setup, prediction_X_cols_sftd, keep_rows=dfrel_setup['wi_trial_keep'])\n",
    "# y_setup_noiti = get_y(dfrel_setup, y_col, keep_rows=dfrel_setup['wi_trial_keep'])\n",
    "# best_score, best_score_std, best_params, best_model, cv_results = sglm_ez.simple_cv_fit(X_setup, y_setup, kfold_cv_idx, glm_kwarg_lst, model_type='Normal', verbose=0, score_method=score_method)\n",
    "\n",
    "# sglm_ez.print_best_model_info(X_setup, best_score, best_params, best_model, start)\n",
    "\n",
    "# X_holdout_witi = get_x(dfrel_holdout, prediction_X_cols_sftd, keep_rows=None)\n",
    "# y_holdout_witi = get_y(dfrel_holdout, y_col, keep_rows=None)\n",
    "# X_holdout_noiti = get_x(dfrel_holdout, prediction_X_cols_sftd, keep_rows=dfrel_holdout['wi_trial_keep'])\n",
    "# y_holdout_noiti = get_y(dfrel_holdout, y_col, keep_rows=dfrel_holdout['wi_trial_keep'])\n",
    "# glm, holdout_score, holdout_neg_mse_score = sglm_ez.training_fit_holdout_score(X_setup, y_setup, X_holdout_noiti, y_holdout_noiti, best_params)\n",
    "\n",
    "# dfrel['pred'] = glm.predict(dfrel[prediction_X_cols_sftd])\n",
    "# dfrel_setup['pred'] = glm.predict(dfrel_setup[prediction_X_cols_sftd])\n",
    "# dfrel_holdout['pred'] = glm.predict(dfrel_holdout[prediction_X_cols_sftd])\n",
    "\n",
    "# # Collect\n",
    "# results_dict[f'{run_id}'] = {'holdout_score':holdout_score,\n",
    "#                             'holdout_neg_mse_score':holdout_neg_mse_score,\n",
    "#                             'best_score':best_score,\n",
    "#                             'best_params':best_params,\n",
    "#                             'all_models':sorted([(_['cv_R2_score'],\n",
    "#                                                     _['cv_mse_score'],\n",
    "#                                                     sglm_ez.calc_l1(_['cv_coefs']),\n",
    "#                                                     sglm_ez.calc_l2(_['cv_coefs']),\n",
    "#                                                     _['glm_kwargs']) for _ in cv_results['full_cv_results']], key=lambda x: -x[0])\n",
    "#                             }\n",
    "\n",
    "# X_cols_plot = prediction_X_cols\n",
    "# X_cols_sftd_plot = prediction_X_cols_sftd\n",
    "\n",
    "# # print('X_setup.columns', list(X_setup.columns), len(list(X_setup.columns)))\n",
    "# # print('X_setup_noiti.columns', list(X_setup_noiti.columns), len(list(X_setup_noiti.columns)))\n",
    "# # print('X_holdout_witi.columns', list(X_holdout_witi.columns), len(list(X_holdout_witi.columns)))\n",
    "# # print('X_holdout_noiti.columns', list(X_holdout_noiti.columns), len(list(X_holdout_noiti.columns)))\n",
    "\n",
    "\n",
    "# holdout_score_rnd = np.round(holdout_score, 4)\n",
    "# best_beta_fn = f'{best_coeffs_folder}/{run_id}_best_{all_betas_basename}_R2_{holdout_score_rnd}.png'\n",
    "# splt.plot_all_beta_coefs(glm.coef_, X_cols_plot,\n",
    "#                                 X_cols_sftd_plot,\n",
    "#                                 plot_width=4,\n",
    "#                                 # plot_width=2,\n",
    "#                                 y_lims=(-2.5, 2.5),\n",
    "#                                 # filename=f'{fn}_coeffs.png',\n",
    "#                                 binsize=54,\n",
    "#                                 filename=best_beta_fn,\n",
    "#                                 plot_name=f'Best Coeffs - {run_id} — {best_params}'\n",
    "#                                 )\n",
    "\n",
    "# best_beta_fn = f'{best_reconstruct_folder}/{run_id}_best_{avg_reconstruct_basename}_R2_{holdout_score_rnd}.png'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# splt.plot_avg_reconstructions_v2(dfrel_holdout,\n",
    "# # splt.plot_avg_reconstructions_v2(dfrel,\n",
    "#                             channel=y_col,\n",
    "#                             binsize = 54,\n",
    "#                             plot_width=4,\n",
    "#                             min_time = -20,\n",
    "#                             max_time = 30,\n",
    "#                             min_signal = -3.0,\n",
    "#                             max_signal = 3.0,\n",
    "#                             file_name=best_beta_fn,\n",
    "#                             title=f'Best Average Reconstruction - {run_id} — {best_params}'\n",
    "#                             )\n",
    "\n",
    "# for fitted_model_dict in (cv_results['full_cv_results']):\n",
    "#     fitted_model = fitted_model_dict['model']\n",
    "#     kwarg_info = \"_\".join([f\"{_k}_{fitted_model_dict['glm_kwargs'][_k]}\" for _k in fitted_model_dict[\"glm_kwargs\"]])\n",
    "\n",
    "#     model_coef = fitted_model.coef_\n",
    "#     model_intercept = fitted_model.intercept_\n",
    "\n",
    "#     std_name = f'{run_id}_{kwarg_info}'\n",
    "#     np.save(f'{all_models_folder}/coeffs/{std_name}_{model_c_basename}.npy', model_coef)\n",
    "#     np.save(f'{all_models_folder}/intercepts/{std_name}_{model_i_basename}.npy', model_intercept)\n",
    "    \n",
    "#     tmp_holdout_score = fitted_model.r2_score(X_holdout_noiti, y_holdout_noiti)\n",
    "\n",
    "#     glmsave.append_fit_results(y_col, fitted_model_dict[\"glm_kwargs\"], glm_model=fitted_model, dropped_cols=left_out,\n",
    "#                             scores={\n",
    "#                                 'tr_witi':fitted_model.r2_score(X_setup, y_setup),\n",
    "#                                 'tr_noiti':fitted_model.r2_score(X_setup_noiti, y_setup_noiti),\n",
    "#                                 'gss_witi':fitted_model_dict['cv_R2_score'],\n",
    "#                                 'gss_noiti':None,\n",
    "#                                 'holdout_witi':fitted_model.r2_score(X_holdout_witi, y_holdout_witi),\n",
    "#                                 'holdout_noiti':fitted_model.r2_score(X_holdout_noiti, y_holdout_noiti)\n",
    "#                             },\n",
    "#                             gssids=kfold_cv_idx)\n",
    "\n",
    "#     tmp = dfrel_holdout.set_index('nTrial').copy()\n",
    "#     tmp['pred'] = fitted_model.predict(get_x(dfrel_holdout, prediction_X_cols_sftd, keep_rows=None))\n",
    "#     tmp = lpp.get_first_entry_time(tmp)\n",
    "#     tmp_y = get_y(dfrel_holdout, y_col, keep_rows=None).copy()\n",
    "#     tmp_y.index = tmp.index\n",
    "#     tmp[y_holdout_noiti.name] = tmp_y\n",
    "\n",
    "#     tmp.to_csv(f'{all_data_folder}/{std_name}_{tmp_data_basename}.csv')\n",
    "\n",
    "#     holdout_score_rnd = np.round(tmp_holdout_score, 4)\n",
    "\n",
    "\n",
    "#     splt.plot_all_beta_coefs(fitted_model.coef_, X_cols_plot,\n",
    "#                                     X_cols_sftd_plot,\n",
    "#                                     plot_width=4,\n",
    "#                                     y_lims=(-3.0, 3.0),\n",
    "#                                     # filename=f'{fn}_coeffs.png',\n",
    "#                                     binsize=54,\n",
    "#                                     filename=f'{all_coeffs_folder}/{std_name}_{all_betas_basename}_R2_{holdout_score_rnd}.png',\n",
    "#                                     plot_name=f'Coeffs by Timeshift - {run_id} — {kwarg_info}'\n",
    "#                                     # plot_name=f'{fn} — {y_col} — {kwarg_info}'\n",
    "#                                     )\n",
    "    \n",
    "#     plt.close('all')\n",
    "# plt.close('all')\n",
    "\n",
    "\n",
    "# glmsave.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = df[['nTrial', 'r_trial', 'nr_trial', 'photometrySideInIndexr', 'photometrySideInIndexnr', 'photometryCenterInIndex']]\n",
    "# t.loc[2295:2345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(list(set([_.split('_')[-1].split('.')[0] for _ in glob.glob('/Users/josh/Documents/Harvard/GLM/sabatinilab-glm/sglm/data/raw/fig3/*')])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_fn = signal_files[file_num].split('/')[-1]\n",
    "table_fn = table_files[file_num].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "g1 = sorted(glob.glob('/Users/josh/Documents/Harvard/GLM/sabatinilab-glm/sglm/data/interim/*/*.csv'))\n",
    "g1 = [_ for _ in g1 if '-bu/' not in _]\n",
    "g2 = sorted(glob.glob('/Users/josh/Documents/Harvard/GLM/sabatinilab-glm/sglm/data/interim/*/*/*.csv'))\n",
    "g2 = [_ for _ in g2 if '-bu/' not in _]\n",
    "for tf in g1:\n",
    "    tmptf = '/'.join(tf.split('/')[-2:]).replace('.csv', '')\n",
    "    tmptf = '/'.join(tmptf.split('/')[-2:]).replace('GLM_TABLE_', '')\n",
    "    tmptf = '/'.join(tmptf.split('/')[-2:]).replace('GLM_SIGNALS_', '')\n",
    "    tmptf = tmptf.replace('INTERIM_','')\n",
    "    print(tmptf)\n",
    "\n",
    "for tf in g2:\n",
    "    tmptf = '/'.join(tf.split('/')[-3:]).replace('.csv', '')\n",
    "    tmptf = '/'.join(tmptf.split('/')[-3:]).replace('GLM_TABLE_', '')\n",
    "    tmptf = '/'.join(tmptf.split('/')[-3:]).replace('GLM_SIGNALS_', '')\n",
    "    tmptf = tmptf.replace('INTERIM_','')\n",
    "    print(tmptf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "app = []\n",
    "for tf in sorted(glob.glob('/Users/josh/Documents/Harvard/GLM/sabatinilab-glm/sglm/data/raw/fig4/*.txt')):\n",
    "    app.append(tf.split('/')[-1].replace('.txt', '').split('_')[-1])\n",
    "\n",
    "print(sorted(list(set(app))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv('/Users/josh/Documents/Harvard/GLM/sabatinilab-glm/sglm/data/raw/fig5/GLM_SIGNALS_S1449_06022022.txt')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv('/Users/josh/Documents/Harvard/GLM/sabatinilab-glm/sglm/data/raw/fig5/GLM_TABLE_S1449_06022022.txt').drop('word',axis=1).astype(float)\n",
    "# tmp[(tmp == 19821).sum(axis=1) >= 1]\n",
    "tmp[(tmp[['photometrySideInIndex']] <= 20000).sum(axis=1) >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa0fc083a9a7b25dab36cbe71fb89b2f1907d4eced1698b208dea6977346b521"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
