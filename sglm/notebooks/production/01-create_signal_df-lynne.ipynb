{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WT61',)\n",
      "> GLM_SIGNALS_WT61_10042021.txt\n",
      "> GLM_SIGNALS_WT61_10062021.txt\n",
      "> GLM_SIGNALS_WT61_10132021.txt\n",
      "> GLM_SIGNALS_WT61_10182021.txt\n",
      "> GLM_SIGNALS_WT61_10112021.txt\n",
      "('WT64',)\n",
      "> GLM_SIGNALS_WT64_11122021.txt\n",
      "> GLM_SIGNALS_WT64_11102021.txt\n",
      "> GLM_SIGNALS_WT64_11082021.txt\n",
      "> GLM_SIGNALS_WT64_11182021.txt\n",
      "> GLM_SIGNALS_WT64_11222021.txt\n",
      "> GLM_SIGNALS_WT64_11162021.txt\n",
      "('WT63',)\n",
      "> GLM_SIGNALS_WT63_11122021.txt\n",
      "> GLM_SIGNALS_WT63_11102021.txt\n",
      "> GLM_SIGNALS_WT63_11082021.txt\n",
      "> GLM_SIGNALS_WT63_11182021.txt\n",
      "> GLM_SIGNALS_WT63_11222021.txt\n",
      "> GLM_SIGNALS_WT63_11162021.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36c656e01fb4b3293891f0cf21b0a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=17.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of iterations 2 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 3 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 3 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 4 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 2 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 7 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 6 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 5 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 5 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 3 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 4 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 4 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 4 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 3 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 4 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 3 — Final max amount of duplicated Center Out Indices: 1\n",
      "# of iterations 4 — Final max amount of duplicated Center Out Indices: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sglm.models import sglm\n",
    "from sglm.features import gen_signal_df as gsd\n",
    "from sglm.features import build_features as bf\n",
    "from sglm.features import gen_signal_df as gsd\n",
    "from sglm.features import build_features as bf\n",
    "\n",
    "y_col_lst_all = ['gACH', 'rDA', 'gDA', 'Ch5', 'Ch6', 'GP_1', 'GP_2', 'GP_5', 'GP_6', 'SGP_1', 'SGP_2', 'SGP_5', 'SGP_6']\n",
    "\n",
    "# Load Signal Data\n",
    "signal_files = glob.glob(f'../../data/raw/GLM_SIGNALS_WT61_*')\n",
    "signal_files += glob.glob(f'../../data/raw/GLM_SIGNALS_WT63_*')\n",
    "signal_files += glob.glob(f'../../data/raw/GLM_SIGNALS_WT64_*')\n",
    "ignore_files = [\n",
    "                'WT61_10152021',\n",
    "                'WT61_10082021'\n",
    "                ]\n",
    "for ign in ignore_files:\n",
    "    signal_files = [_ for _ in signal_files if ign not in _]\n",
    "\n",
    "table_files = [_.replace('GLM_SIGNALS', 'GLM_TABLE') for _ in signal_files]\n",
    "\n",
    "channel_definitions = {\n",
    "        ('WT61',): {'Ch1': 'gACH', 'Ch2': 'rDA'},\n",
    "        ('WT64',): {'Ch1': 'gACH', 'Ch2': 'empty'},\n",
    "        ('WT63',): {'Ch1': 'gDA', 'Ch2': 'empty'},\n",
    "    }\n",
    "channel_assignments = bf.get_rename_columns_by_file(signal_files, channel_definitions)\n",
    "\n",
    "for file_num in trange(len(signal_files)):\n",
    "\n",
    "    ## Load Table Data\n",
    "    # signal_fn = signal_files[0]\n",
    "    # table_fn = table_files[0]\n",
    "\n",
    "    signal_path = signal_files[file_num]\n",
    "    table_path = table_files[file_num]\n",
    "\n",
    "    signal_fn = signal_files[file_num].split('/')[-1]\n",
    "    table_fn = table_files[file_num].split('/')[-1]\n",
    "\n",
    "    signal_filename_out = signal_fn.replace('GLM_SIGNALS', 'GLM_SIGNALS_INTERIM').replace('txt', 'csv')\n",
    "    table_filename_out = table_fn.replace('GLM_TABLE', 'GLM_TABLE_INTERIM').replace('txt', 'csv')\n",
    "\n",
    "    signal_path_out = f'../../data/interim/{signal_filename_out}'\n",
    "    table_path_out = f'../../data/interim/{table_filename_out}'\n",
    "\n",
    "\n",
    "    signal_df = pd.read_csv(signal_path)\n",
    "    table_df = pd.read_csv(table_path)\n",
    "\n",
    "    signal_df, table_df = gsd.generate_signal_df(signal_path,\n",
    "                                            table_path,\n",
    "                                            # signal_filename_out=f'../../data/interim/{signal_filename_out}',\n",
    "                                            # table_filename_out=f'../../data/interim/{table_filename_out}'\n",
    "                                            )\n",
    "\n",
    "    signal_df = signal_df[signal_df['nTrial'] > 0].fillna(0)\n",
    "\n",
    "    # Break down Preprocess Lynne into component parts\n",
    "\n",
    "    # Rename Columns\n",
    "    signal_df = bf.rename_consistent_columns(signal_df)\n",
    "\n",
    "    for y_col in y_col_lst_all:\n",
    "        if y_col not in signal_df.columns:\n",
    "            signal_df[y_col] = np.nan\n",
    "            continue\n",
    "\n",
    "    # print(channel_assignments.keys())\n",
    "    # print(signal_fn)\n",
    "    if signal_fn in channel_assignments:\n",
    "        signal_df = signal_df.rename(channel_assignments[signal_fn], axis=1)\n",
    "\n",
    "    ## Set Full Trial Reward Flags\n",
    "    signal_df['r_trial'] = (signal_df.groupby('nTrial')['photometrySideInIndexr'].transform(np.sum) > 0) * 1.0\n",
    "    signal_df['nr_trial'] = (signal_df.groupby('nTrial')['photometrySideInIndexnr'].transform(np.sum) > 0) * 1.0\n",
    "\n",
    "    ## Define Side Rewarded / Unrewarded Flags\n",
    "    signal_df = bf.set_port_entry_exit_rewarded_unrewarded_indicators(signal_df)\n",
    "\n",
    "    ## Define Side Agnostic Events\n",
    "    signal_df = bf.define_side_agnostic_events(signal_df)\n",
    "\n",
    "    # print('Percent of Data in ITI:', (df['nTrial'] == df['nEndTrial']).mean())\n",
    "\n",
    "    signal_df['spnrOff'] = ((signal_df['spnr'] == 1)&(signal_df['photometrySideInIndex'] != 1)).astype(int)\n",
    "    signal_df['spxrOff'] = ((signal_df['spxr'] == 1)&(signal_df['photometrySideOutIndex'] != 1)).astype(int)\n",
    "    spnnrOff_a = ((signal_df['spnnr'] == 1)&(signal_df['photometrySideInIndex'] != 1)).astype(int)\n",
    "    spxnrOff_a = ((signal_df['spxnr'] == 1)&(signal_df['photometrySideOutIndex'] != 1)).astype(int)\n",
    "\n",
    "    # If we have something listed as a rewarded \"off\" side entry labeled in the table as a side exit... it means it was a fast \"out-in\".\n",
    "    # The latter \"in\" should be considered an unrewarded side port \"off\" entry.\n",
    "    dualism_exen = ((signal_df['spnrOff'] == 1)&(signal_df['photometrySideOutIndex'] == 1)).astype(int)\n",
    "\n",
    "    # Unrewarded side port entries should be the combination of those simply identified by checking spnnr & the table labels +\n",
    "    # the dualism defined immediately prior. Then those dualism examples should be remoed from the \"off\" rewarded entries.\n",
    "    signal_df['spnnrOff'] = spnnrOff_a + dualism_exen\n",
    "    signal_df['spnrOff'] = signal_df['spnrOff'] - dualism_exen\n",
    "\n",
    "    signal_df['spxnrOff'] = spxnrOff_a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    signal_df['cpnOff'] = ((signal_df['cpn'] == 1)&(signal_df['photometryCenterInIndex'] != 1)).astype(int)\n",
    "    signal_df['cpxOff'] = ((signal_df['cpx'] == 1)&(signal_df['photometryCenterOutIndex'] != 1)).astype(int)\n",
    "    # spnnrOff_a = ((signal_df['cpnOff'] == 1)&(signal_df['photometryCenterInIndex'] != 1)).astype(int)\n",
    "    # spxnrOff_a = ((signal_df['cpxOff'] == 1)&(signal_df['photometryCenterOutIndex'] != 1)).astype(int)\n",
    "\n",
    "    # # If we have something listed as a rewarded \"off\" side entry labeled in the table as a side exit... it means it was a fast \"out-in\".\n",
    "    # # The latter \"in\" should be considered an unrewarded side port \"off\" entry.\n",
    "    # dualism_exen = ((signal_df['cpnOff'] == 1)&(signal_df['photometryCenterOutIndex'] == 1)).astype(int)\n",
    "\n",
    "    # # Unrewarded side port entries should be the combination of those simply identified by checking spnnr & the table labels +\n",
    "    # # the dualism defined immediately prior. Then those dualism examples should be remoed from the \"off\" rewarded entries.\n",
    "    # signal_df['spnnrOff'] = spnnrOff_a + dualism_exen\n",
    "    # signal_df['spnrOff'] = signal_df['spnrOff'] - dualism_exen\n",
    "\n",
    "    # signal_df['spxnrOff'] = spxnrOff_a\n",
    "\n",
    "\n",
    "    if signal_path_out:\n",
    "        signal_df.to_csv(signal_path_out, index_label='index')\n",
    "    if table_path_out:\n",
    "        table_df.to_csv(table_path_out, index_label='index')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df['spnnr'] = ((df['spnnr'] == 1)&(df['photometrySideInIndex'] != 1)).astype(int)\n",
    "# # df['spxnr'] = ((df['spxnr'] == 1)&(df['photometrySideOutIndex'] != 1)).astype(int)\n",
    "\n",
    "# X_cols = [_ for _ in X_cols_all if _ not in left_out]\n",
    "\n",
    "# if len(leave_one_out_list) > 1:\n",
    "#     run_id = f'{prefix}_{fn}_{y_col}_drop={\"_\".join(left_out)}'\n",
    "# else:\n",
    "#     run_id = f'{prefix}_{fn}_{y_col}'\n",
    "\n",
    "# dfrel = df.copy()\n",
    "\n",
    "# dfrel, X_cols_sftd = lpp.timeshift_vals(dfrel, X_cols, neg_order=neg_order, pos_order=pos_order)\n",
    "\n",
    "# dfrel_setup, dfrel_holdout = holdout_splits(dfrel,\n",
    "#                                             id_cols=['nTrial'],\n",
    "#                                             perc_holdout=pholdout)\n",
    "# dfrel_setup, dfrel_holdout = dfrel_setup.copy(), dfrel_holdout.copy()\n",
    "\n",
    "# kfold_cv_idx = sglm_ez.cv_idx_by_trial_id(dfrel_setup,\n",
    "#                                           trial_id_columns=['nTrial'],\n",
    "#                                           num_folds=folds,\n",
    "#                                           test_size=pgss)\n",
    "\n",
    "# prediction_X_cols = [_ for _ in X_cols if _ not in ['nTrial']]\n",
    "# prediction_X_cols_sftd = [_ for _ in X_cols_sftd if _ not in ['nTrial']]\n",
    "\n",
    "# X_setup = get_x(dfrel_setup, prediction_X_cols_sftd, keep_rows=None)\n",
    "# y_setup = get_y(dfrel_setup, y_col, keep_rows=None)\n",
    "# X_setup_noiti = get_x(dfrel_setup, prediction_X_cols_sftd, keep_rows=dfrel_setup['wi_trial_keep'])\n",
    "# y_setup_noiti = get_y(dfrel_setup, y_col, keep_rows=dfrel_setup['wi_trial_keep'])\n",
    "# best_score, best_score_std, best_params, best_model, cv_results = sglm_ez.simple_cv_fit(X_setup, y_setup, kfold_cv_idx, glm_kwarg_lst, model_type='Normal', verbose=0, score_method=score_method)\n",
    "\n",
    "# sglm_ez.print_best_model_info(X_setup, best_score, best_params, best_model, start)\n",
    "\n",
    "# X_holdout_witi = get_x(dfrel_holdout, prediction_X_cols_sftd, keep_rows=None)\n",
    "# y_holdout_witi = get_y(dfrel_holdout, y_col, keep_rows=None)\n",
    "# X_holdout_noiti = get_x(dfrel_holdout, prediction_X_cols_sftd, keep_rows=dfrel_holdout['wi_trial_keep'])\n",
    "# y_holdout_noiti = get_y(dfrel_holdout, y_col, keep_rows=dfrel_holdout['wi_trial_keep'])\n",
    "# glm, holdout_score, holdout_neg_mse_score = sglm_ez.training_fit_holdout_score(X_setup, y_setup, X_holdout_noiti, y_holdout_noiti, best_params)\n",
    "\n",
    "# dfrel['pred'] = glm.predict(dfrel[prediction_X_cols_sftd])\n",
    "# dfrel_setup['pred'] = glm.predict(dfrel_setup[prediction_X_cols_sftd])\n",
    "# dfrel_holdout['pred'] = glm.predict(dfrel_holdout[prediction_X_cols_sftd])\n",
    "\n",
    "# # Collect\n",
    "# results_dict[f'{run_id}'] = {'holdout_score':holdout_score,\n",
    "#                             'holdout_neg_mse_score':holdout_neg_mse_score,\n",
    "#                             'best_score':best_score,\n",
    "#                             'best_params':best_params,\n",
    "#                             'all_models':sorted([(_['cv_R2_score'],\n",
    "#                                                     _['cv_mse_score'],\n",
    "#                                                     sglm_ez.calc_l1(_['cv_coefs']),\n",
    "#                                                     sglm_ez.calc_l2(_['cv_coefs']),\n",
    "#                                                     _['glm_kwargs']) for _ in cv_results['full_cv_results']], key=lambda x: -x[0])\n",
    "#                             }\n",
    "\n",
    "# X_cols_plot = prediction_X_cols\n",
    "# X_cols_sftd_plot = prediction_X_cols_sftd\n",
    "\n",
    "# # print('X_setup.columns', list(X_setup.columns), len(list(X_setup.columns)))\n",
    "# # print('X_setup_noiti.columns', list(X_setup_noiti.columns), len(list(X_setup_noiti.columns)))\n",
    "# # print('X_holdout_witi.columns', list(X_holdout_witi.columns), len(list(X_holdout_witi.columns)))\n",
    "# # print('X_holdout_noiti.columns', list(X_holdout_noiti.columns), len(list(X_holdout_noiti.columns)))\n",
    "\n",
    "\n",
    "# holdout_score_rnd = np.round(holdout_score, 4)\n",
    "# best_beta_fn = f'{best_coeffs_folder}/{run_id}_best_{all_betas_basename}_R2_{holdout_score_rnd}.png'\n",
    "# splt.plot_all_beta_coefs(glm.coef_, X_cols_plot,\n",
    "#                                 X_cols_sftd_plot,\n",
    "#                                 plot_width=4,\n",
    "#                                 # plot_width=2,\n",
    "#                                 y_lims=(-2.5, 2.5),\n",
    "#                                 # filename=f'{fn}_coeffs.png',\n",
    "#                                 binsize=54,\n",
    "#                                 filename=best_beta_fn,\n",
    "#                                 plot_name=f'Best Coeffs - {run_id} — {best_params}'\n",
    "#                                 )\n",
    "\n",
    "# best_beta_fn = f'{best_reconstruct_folder}/{run_id}_best_{avg_reconstruct_basename}_R2_{holdout_score_rnd}.png'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# splt.plot_avg_reconstructions_v2(dfrel_holdout,\n",
    "# # splt.plot_avg_reconstructions_v2(dfrel,\n",
    "#                             channel=y_col,\n",
    "#                             binsize = 54,\n",
    "#                             plot_width=4,\n",
    "#                             min_time = -20,\n",
    "#                             max_time = 30,\n",
    "#                             min_signal = -3.0,\n",
    "#                             max_signal = 3.0,\n",
    "#                             file_name=best_beta_fn,\n",
    "#                             title=f'Best Average Reconstruction - {run_id} — {best_params}'\n",
    "#                             )\n",
    "\n",
    "# for fitted_model_dict in (cv_results['full_cv_results']):\n",
    "#     fitted_model = fitted_model_dict['model']\n",
    "#     kwarg_info = \"_\".join([f\"{_k}_{fitted_model_dict['glm_kwargs'][_k]}\" for _k in fitted_model_dict[\"glm_kwargs\"]])\n",
    "\n",
    "#     model_coef = fitted_model.coef_\n",
    "#     model_intercept = fitted_model.intercept_\n",
    "\n",
    "#     std_name = f'{run_id}_{kwarg_info}'\n",
    "#     np.save(f'{all_models_folder}/coeffs/{std_name}_{model_c_basename}.npy', model_coef)\n",
    "#     np.save(f'{all_models_folder}/intercepts/{std_name}_{model_i_basename}.npy', model_intercept)\n",
    "    \n",
    "#     tmp_holdout_score = fitted_model.r2_score(X_holdout_noiti, y_holdout_noiti)\n",
    "\n",
    "#     glmsave.append_fit_results(y_col, fitted_model_dict[\"glm_kwargs\"], glm_model=fitted_model, dropped_cols=left_out,\n",
    "#                             scores={\n",
    "#                                 'tr_witi':fitted_model.r2_score(X_setup, y_setup),\n",
    "#                                 'tr_noiti':fitted_model.r2_score(X_setup_noiti, y_setup_noiti),\n",
    "#                                 'gss_witi':fitted_model_dict['cv_R2_score'],\n",
    "#                                 'gss_noiti':None,\n",
    "#                                 'holdout_witi':fitted_model.r2_score(X_holdout_witi, y_holdout_witi),\n",
    "#                                 'holdout_noiti':fitted_model.r2_score(X_holdout_noiti, y_holdout_noiti)\n",
    "#                             },\n",
    "#                             gssids=kfold_cv_idx)\n",
    "\n",
    "#     tmp = dfrel_holdout.set_index('nTrial').copy()\n",
    "#     tmp['pred'] = fitted_model.predict(get_x(dfrel_holdout, prediction_X_cols_sftd, keep_rows=None))\n",
    "#     tmp = lpp.get_first_entry_time(tmp)\n",
    "#     tmp_y = get_y(dfrel_holdout, y_col, keep_rows=None).copy()\n",
    "#     tmp_y.index = tmp.index\n",
    "#     tmp[y_holdout_noiti.name] = tmp_y\n",
    "\n",
    "#     tmp.to_csv(f'{all_data_folder}/{std_name}_{tmp_data_basename}.csv')\n",
    "\n",
    "#     holdout_score_rnd = np.round(tmp_holdout_score, 4)\n",
    "\n",
    "\n",
    "#     splt.plot_all_beta_coefs(fitted_model.coef_, X_cols_plot,\n",
    "#                                     X_cols_sftd_plot,\n",
    "#                                     plot_width=4,\n",
    "#                                     y_lims=(-3.0, 3.0),\n",
    "#                                     # filename=f'{fn}_coeffs.png',\n",
    "#                                     binsize=54,\n",
    "#                                     filename=f'{all_coeffs_folder}/{std_name}_{all_betas_basename}_R2_{holdout_score_rnd}.png',\n",
    "#                                     plot_name=f'Coeffs by Timeshift - {run_id} — {kwarg_info}'\n",
    "#                                     # plot_name=f'{fn} — {y_col} — {kwarg_info}'\n",
    "#                                     )\n",
    "    \n",
    "#     plt.close('all')\n",
    "# plt.close('all')\n",
    "\n",
    "\n",
    "# glmsave.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = df[['nTrial', 'r_trial', 'nr_trial', 'photometrySideInIndexr', 'photometrySideInIndexnr', 'photometryCenterInIndex']]\n",
    "# t.loc[2295:2345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa0fc083a9a7b25dab36cbe71fb89b2f1907d4eced1698b208dea6977346b521"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
