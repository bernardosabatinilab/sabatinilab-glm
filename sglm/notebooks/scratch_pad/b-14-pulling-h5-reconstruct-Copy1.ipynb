{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sglm.helpers import filehelpers as fh, dfhelpers as dfh\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_keys = list({\n",
    "                'gACH':(0,0),\n",
    "                'rDA':(0,0),\n",
    "                'gDA':(0,0),\n",
    "                'photometrySideInIndexr':(0,0),\n",
    "                'photometrySideInIndexnr':(0,0),\n",
    "                'photometrySideInIndexAA':(0,0),\n",
    "                'photometrySideInIndexAa':(0,0),\n",
    "                'photometrySideInIndexaA':(0,0),\n",
    "                'photometrySideInIndexaa':(0,0),\n",
    "                'photometrySideInIndexAB':(0,0),\n",
    "                'photometrySideInIndexAb':(0,0),\n",
    "                'photometrySideInIndexaB':(0,0),\n",
    "                'photometrySideInIndexab':(0,0),\n",
    "                }.keys())\n",
    "shortened_keys = [dfh.shorten_col_name(_) for _ in base_keys]\n",
    "\n",
    "all_alignment_cols = [\n",
    "                        'SIAA', 'SIAa', 'SIaA', 'SIaa',\n",
    "                        'SIAB', 'SIAb', 'SIaB', 'SIab',\n",
    "                        'SIr', 'SInr', \n",
    "                     ]\n",
    "# base_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_list(event_srs, event_col, keep_srs):\n",
    "    \n",
    "    keep_eg_lst = []\n",
    "    delta_keep_eg_lst = []\n",
    "\n",
    "    rdc = []\n",
    "    entry_num = np.arange(len(event_srs))\n",
    "    event = np.where(event_srs.values == 1)\n",
    "    assert len(event) == 1\n",
    "    event = event[0]\n",
    "    \n",
    "    for iev, ev in enumerate(event):\n",
    "        delta = entry_num - ev\n",
    "        keep_eg = (delta >= -40)&(delta <= 100)&keep_srs\n",
    "        delta_keep_eg = delta[keep_eg]\n",
    "        keep_eg_lst.append(keep_eg)\n",
    "        delta_keep_eg_lst.append(delta_keep_eg)\n",
    "\n",
    "    return keep_eg_lst, delta_keep_eg_lst\n",
    "\n",
    "# full_df = h5_lst[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fig_folder ('Figure_6_g1--20_20sft', 'glu_run_20220109--20_20sft_0-ft')\n",
      "gGLUr_0_0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIAA\n",
      "0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dabb00980df4608afc73ec4efc02f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIAa\n",
      "0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4fe0d4117f46f5ba7745a844bcdab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIaA\n",
      "0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5600a576fc87404899e1cc13a56777c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIaa\n",
      "0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52add496b1fb4026a7ecca0419ed3598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIAB\n",
      "0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec9d19d6555483fa5ebd1132891ca89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIAb\n",
      "0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27999b3a17da43ec9e6fd0a39771529f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIaB\n",
      "0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fc231e3ebf4bde982e0452867736df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIab\n",
      "0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989b45fb3eb24b08834dbc7e867cee94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIr\n",
      "0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab4f64979524b958af8a18bba6ad440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1581 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SInr\n",
      "0_base_simple_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73976b6476e4419af0bacf4f656ab9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIAA\n",
      "1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c786c8751d464e259fa79a184f0f7563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIAa\n",
      "1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76d20b2bb18486c808038f1e1a95335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIaA\n",
      "1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e6afe2d6814fc9b9979fc6ec54fd38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIaa\n",
      "1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a74f92d41154951a45a488df7125a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIAB\n",
      "1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e242d627c5401e8f955a6d450ffacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIAb\n",
      "1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9969f4e69f6d4b6ab30a38247296fc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIaB\n",
      "1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ef6292fa6641eebac732b036e2e956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIab\n",
      "1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d990df8d83c74fb9b3b56d4665f0fada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SIr\n",
      "1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a3ab87ad7242409a305e3ea59c33a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1581 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n",
      "gGLUr_0_1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0_run_num__0 SInr\n",
      "1_base_words_max_iter10000__fit_interceptFalse__alpha0____0__l1_ratio0____0\n",
      "Pulling delta list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b368c113052e423389f0ff33d3d57f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys dict_keys(['true', 'pred'])\n"
     ]
    }
   ],
   "source": [
    "# base_location = Path(r'/Users/josh/Documents/Harvard/GLM/sabatinilab-glm/sglm/outputs')\n",
    "base_location = Path(r'C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\outputs')\n",
    "\n",
    "# fig_folder_lst = [('Figure_1_2', 'f1*-ft'),\n",
    "#                   ('Figure_3',   'f3*-ft'),\n",
    "#                   ('Figure_4_g1',   'f4*-ft'),\n",
    "#                   ('Figure_4_g2',   'f4*-ft'),\n",
    "#                   ('Figure_5_g1',   'f5*-ft'),\n",
    "#                   ('Figure_5_g2',   'f5*-ft'),\n",
    "#                   ('Figure_5_g5',   'f5*-ft'),\n",
    "#                  ]\n",
    "fig_folder_lst = [#('Figure_1_2', 'f1-n*-ft'),\n",
    "#                   ('Figure_3',   'f3*-ft'),\n",
    "#                   ('Figure_4_g1',   'f4*-ft'),\n",
    "#                   ('Figure_4_g2',   'f4*-ft'),\n",
    "#                   ('Figure_5_g1',   'f5*-ft'),\n",
    "#                   ('Figure_5_g2',   'f5*-ft'),\n",
    "#                   ('Figure_5_g5',   'f5*-ft'),\n",
    "#                   ('Figure_6_g1',   'glu*-ft'),\n",
    "#                     ('Figure_6_g1', 'glu_run_20221202-30sft-reduc_0-ft')\n",
    "#                     ('Figure_6_g1-20sft', 'glu_run_20221212-20sft-paperfig_0-ft')\n",
    "#                     ('Figure_6_g1-50sft', 'glu_run_20221213-50sft-paperfig_0-ft')\n",
    "                    ('Figure_6_g1--20_20sft', 'glu_run_20220109--20_20sft_0-ft')\n",
    "                 ]\n",
    "\n",
    "for fig_folder in fig_folder_lst:\n",
    "    print('fig_folder', fig_folder)\n",
    "    \n",
    "    load_folder = base_location / Path(fig_folder[0])\n",
    "    # load_folder = base_location / Path(r'tmp')\n",
    "    h5_recons_locations = str((load_folder / Path('all') / Path(fig_folder[1]) / Path(r'reconstructs\\best_resids_*.h5')).resolve())\n",
    "    \n",
    "#     print(h5_recons_locations)\n",
    "    \n",
    "#     load_folder = base_location / Path(r'Figure_1_2')\n",
    "#     # load_folder = base_location / Path(r'tmp')\n",
    "#     h5_recons_locations = str((load_folder / Path(r'all\\f1*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "#     # h5_recons_locations = str((load_folder / Path(r'all/f1*-ft/reconstructs/best_resids_*.h5')).resolve())\n",
    "#     # load_folder = base_location / Path(r'Figure_3')\n",
    "#     # h5_beta_locations = str((load_folder / Path(r'all\\f3*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "#     # load_folder = base_location / Path(r'Figure_4_g1')\n",
    "#     # h5_beta_locations = str((load_folder / Path(r'all\\f4*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "#     # load_folder = base_location / Path(r'Figure_4_g2')\n",
    "#     # h5_beta_locations = str((load_folder / Path(r'all\\f4*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "#     # load_folder = base_location / Path(r'Figure_5_g1')\n",
    "#     # h5_beta_locations = str((load_folder / Path(r'all\\f5*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "#     # load_folder = base_location / Path(r'Figure_5_g2')\n",
    "#     # h5_beta_locations = str((load_folder / Path(r'all\\f5*-ft\\coefs\\*_best_coeffs.h5')).resolve())\n",
    "#     # load_folder = base_location / Path(r'Figure_5_g5')\n",
    "#     # h5_beta_locations = str((load_folder / Path(r'all\\f5*-ft\\coefs\\*_best_coeffs.h5')).resolve())\n",
    "\n",
    "#     out_folder = base_location / Path(r'final_outputs_rev-msesplt')\n",
    "#     out_folder = base_location / Path(r'final_outputs_rev-reconsplt')\n",
    "    out_folder = base_location / Path(r'final_outputs_rev-reconsplt-paperfig')\n",
    "\n",
    "    out_loc = str((out_folder).resolve())\n",
    "    h5_recons_locations = glob.glob(h5_recons_locations)\n",
    "\n",
    "    fh.create_folder_if_not_exists(out_loc)\n",
    "    \n",
    "    h5_lst = defaultdict(dict)\n",
    "#     run_id_dct = defaultdict(dict)\n",
    "\n",
    "\n",
    "    for ih5, h5_coef_fn in enumerate(h5_recons_locations):\n",
    "        h5f = pd.HDFStore(h5_coef_fn)\n",
    "        h5fk = h5f.keys()\n",
    "        for ik, key in enumerate(h5fk):\n",
    "            key = key.replace('/', '')\n",
    "            resp = key.split('_')[0]\n",
    "            model_version = '_'.join(key.split('_')[3:]).split('_run_num')[0]\n",
    "            \n",
    "\n",
    "            if 'run_num' in key and 'run_num__0' not in key: #and 'run_num__1' not in key:\n",
    "                continue\n",
    "            if 'base_simple' not in model_version and 'base_words' not in model_version and '_to_' not in model_version and 'basis' not in key:\n",
    "                continue\n",
    "            \n",
    "#             print('model_version', model_version)\n",
    "            \n",
    "#             print(key, model_version)\n",
    "#             print(f'ih5, ik, model_version, key, h5_coef_fn - {ih5}, {ik}, {model_version}, {key}, {h5_coef_fn}')\n",
    "\n",
    "            y_col = key.split('_')[0].replace(r'/', r'')\n",
    "            \n",
    "            h5_df = pd.read_hdf(h5f, key)\n",
    "\n",
    "            h5_df.columns = [dfh.shorten_col_name(_) for _ in h5_df.columns]\n",
    "\n",
    "            if 'dfrel_basis' != key.replace('/', ''):\n",
    "                h5_df.columns = [_.split('_')[0] for _ in h5_df.columns]\n",
    "                h5_df['true'] = h5_lst[ih5]['dfrel'][y_col]\n",
    "\n",
    "                h5_df['file_num'] = h5_lst[ih5]['dfrel']['file_num']\n",
    "\n",
    "                h5_df['dupe'] = h5_lst[ih5]['dfrel']['dupe']\n",
    "                h5_df['wi_trial_keep'] = h5_lst[ih5]['dfrel']['wi_trial_keep']\n",
    "\n",
    "                h5_lst[ih5][key] = h5_df\n",
    "            else:\n",
    "                h5_lst[ih5]['dfrel'] = h5_df\n",
    "\n",
    "#     for k in h5_lst:\n",
    "#         for kk in h5_lst[k]:\n",
    "#             print(k, kk)\n",
    "\n",
    "\n",
    "\n",
    "    for ih5 in h5_lst:\n",
    "#     for y_col in ['gACH', 'rDA', 'gDA']:\n",
    "#             fig,axes = plt.subplots(5,2,figsize=(10,30))\n",
    "\n",
    "        for key in h5_lst[ih5]:\n",
    "#         for y_col in ['gACH', 'rDA', 'gDA']:\n",
    "#         for ih5 in h5_lst:\n",
    "            if key == 'dfrel':\n",
    "                continue\n",
    "            \n",
    "            mse_ac = {}\n",
    "            combo_rdc = {}\n",
    "            for iac, alignment_col in enumerate(all_alignment_cols):\n",
    "    #                 ic, ir = iac%2, iac//2\n",
    "    #                 ax = axes[ir, ic]\n",
    "\n",
    "                \n",
    "                resp = key.split('_')[0].replace('/', '')\n",
    "                model_version = '_'.join(key.split('_')[2:]).split('_run_num')[0]\n",
    "                model_version = model_version.replace('5_base_words_gDA_to_gACH', '5_base_words_rDA_to_gACH')\n",
    "                print(key, alignment_col)\n",
    "                print(model_version)\n",
    "#                 break\n",
    "#             break\n",
    "#         break\n",
    "#     break\n",
    "                y_col = key.split('_')[0].replace(r'/', r'')\n",
    "                \n",
    "                dupe = h5_lst[ih5]['dfrel']['dupe']\n",
    "                dfrel_basis = h5_lst[ih5]['dfrel'].loc[~dupe]\n",
    "                dfrel_basis_run = h5_lst[ih5][key].loc[~dupe]\n",
    "\n",
    "                print('Pulling delta list')\n",
    "                keep_rows, assoc_deltas = get_delta_list(dfrel_basis[alignment_col], alignment_col, ((dfrel_basis_run['holdout']==1)).astype(bool))\n",
    "                rdc_lst = defaultdict(list)\n",
    "                for entry_num in trange(len(keep_rows)):\n",
    "                    keep_row_single = keep_rows[entry_num]\n",
    "                    deltas = assoc_deltas[entry_num]\n",
    "                    rdc_df = dfrel_basis_run.loc[keep_row_single].set_index(deltas)\n",
    "\n",
    "                    if rdc_df['file_num'].nunique() > 1:\n",
    "                        continue\n",
    "\n",
    "                    reconstruction_data_list = []\n",
    "\n",
    "                    for bootstrap_col in ['true', 'pred']:\n",
    "                        rdc_lst[bootstrap_col].append(rdc_df[[bootstrap_col]])\n",
    "\n",
    "                print('keys', rdc_lst.keys())\n",
    "                \n",
    "                \n",
    "                \n",
    "                t, p = pd.concat(rdc_lst['true'], axis=1).sort_index(), pd.concat(rdc_lst['pred'], axis=1).sort_index()\n",
    "                dffsq = np.square(t.values - p.values)\n",
    "                mse_ac[alignment_col] = np.sum(dffsq[np.where(~np.isnan(dffsq))])/np.sum(~np.isnan(dffsq))\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                seed = np.random.randint(1000000000)\n",
    "                for bootstrap_col in (rdc_lst):\n",
    "                    tmp_rdc = pd.concat(rdc_lst[bootstrap_col], axis=1).sort_index()\n",
    "\n",
    "                    reconstruction_data_list = []\n",
    "                    for delta in (tmp_rdc.index):\n",
    "                        row = tmp_rdc.loc[delta].dropna().values\n",
    "                        bs = scipy.stats.bootstrap((row,), np.mean, n_resamples=9999,\n",
    "    #                     bs = scipy.stats.bootstrap((row,), np.mean, n_resamples=1000,\n",
    "                                                   batch=None, vectorized=True, paired=False,\n",
    "                                                   axis=0, confidence_level=0.95, method='BCa',\n",
    "                                                   random_state=seed)\n",
    "\n",
    "                        mn_val = row.mean()\n",
    "                        rtup = (bootstrap_col, delta, bs.confidence_interval.low, mn_val, bs.confidence_interval.high)\n",
    "\n",
    "                        reconstruction_data_list.append(rtup)\n",
    "\n",
    "                    tmp_combo_rdc = pd.DataFrame(reconstruction_data_list, columns=['basis_column', 'delta', 'lb', 'mn', 'ub']).set_index('delta')\n",
    "                    combo_rdc[f'{alignment_col}_{bootstrap_col}'] = tmp_combo_rdc.drop('basis_column',axis=1)\n",
    "            \n",
    "            combo_rdc = pd.concat(combo_rdc, axis=1)\n",
    "            combo_rdc.columns = ['_'.join(_).replace('_true_', '_T_').replace('_pred_', '_P_') for _ in combo_rdc.columns]\n",
    "            combo_rdc.to_csv(str((Path(out_loc) / f'{load_folder.parts[-1]}-reconstruct-{y_col}={model_version}.csv').resolve()))\n",
    "            \n",
    "            import json\n",
    "            with open(str((Path(out_loc) / f'{load_folder.parts[-1]}-mse_seg-{y_col}={model_version}.json').resolve()), 'w') as f:\n",
    "                json.dump(mse_ac, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # base_location = Path(r'/Users/josh/Documents/Harvard/GLM/sabatinilab-glm/sglm/outputs')\n",
    "# base_location = Path(r'C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\outputs')\n",
    "\n",
    "# # fig_folder_lst = [('Figure_1_2', 'f1*-ft'),\n",
    "# #                   ('Figure_3',   'f3*-ft'),\n",
    "# #                   ('Figure_4_g1',   'f4*-ft'),\n",
    "# #                   ('Figure_4_g2',   'f4*-ft'),\n",
    "# #                   ('Figure_5_g1',   'f5*-ft'),\n",
    "# #                   ('Figure_5_g2',   'f5*-ft'),\n",
    "# #                   ('Figure_5_g5',   'f5*-ft'),\n",
    "# #                  ]\n",
    "# fig_folder_lst = [#('Figure_1_2', 'f1-n*-ft'),\n",
    "# #                   ('Figure_3',   'f3*-ft'),\n",
    "# #                   ('Figure_4_g1',   'f4*-ft'),\n",
    "# #                   ('Figure_4_g2',   'f4*-ft'),\n",
    "# #                   ('Figure_5_g1',   'f5*-ft'),\n",
    "# #                   ('Figure_5_g2',   'f5*-ft'),\n",
    "# #                   ('Figure_5_g5',   'f5*-ft'),\n",
    "# #                   ('Figure_6_g1',   'glu*-ft'),\n",
    "# #                     ('Figure_6_g1', 'glu_run_20221202-30sft-reduc_0-ft')\n",
    "#                  ]\n",
    "\n",
    "# for fig_folder in fig_folder_lst:\n",
    "#     print('fig_folder', fig_folder)\n",
    "    \n",
    "#     load_folder = base_location / Path(fig_folder[0])\n",
    "#     # load_folder = base_location / Path(r'tmp')\n",
    "#     h5_recons_locations = str((load_folder / Path('all') / Path(fig_folder[1]) / Path(r'reconstructs\\best_resids_*.h5')).resolve())\n",
    "    \n",
    "# #     print(h5_recons_locations)\n",
    "    \n",
    "# #     load_folder = base_location / Path(r'Figure_1_2')\n",
    "# #     # load_folder = base_location / Path(r'tmp')\n",
    "# #     h5_recons_locations = str((load_folder / Path(r'all\\f1*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "# #     # h5_recons_locations = str((load_folder / Path(r'all/f1*-ft/reconstructs/best_resids_*.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_3')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f3*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_4_g1')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f4*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_4_g2')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f4*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_5_g1')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f5*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_5_g2')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f5*-ft\\coefs\\*_best_coeffs.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_5_g5')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f5*-ft\\coefs\\*_best_coeffs.h5')).resolve())\n",
    "\n",
    "# #     out_folder = base_location / Path(r'final_outputs_rev-msesplt')\n",
    "# #     out_folder = base_location / Path(r'final_outputs_rev-reconsplt')\n",
    "#     out_folder = base_location / Path(r'final_outputs_rev-reconsplt-paperfig')\n",
    "\n",
    "#     out_loc = str((out_folder).resolve())\n",
    "#     h5_recons_locations = glob.glob(h5_recons_locations)\n",
    "\n",
    "#     fh.create_folder_if_not_exists(out_loc)\n",
    "    \n",
    "#     h5_lst = defaultdict(dict)\n",
    "# #     run_id_dct = defaultdict(dict)\n",
    "\n",
    "\n",
    "#     for ih5, h5_coef_fn in enumerate(h5_recons_locations):\n",
    "#         h5f = pd.HDFStore(h5_coef_fn)\n",
    "#         h5fk = h5f.keys()\n",
    "#         for ik, key in enumerate(h5fk):\n",
    "#             key = key.replace('/', '')\n",
    "#             resp = key.split('_')[0]\n",
    "#             model_version = '_'.join(key.split('_')[3:]).split('_run_num')[0]\n",
    "            \n",
    "\n",
    "#             if 'run_num' in key and 'run_num__0' not in key: #and 'run_num__1' not in key:\n",
    "#                 continue\n",
    "#             if 'base_simple' not in model_version and 'base_words' not in model_version and '_to_' not in model_version and 'basis' not in key:\n",
    "#                 continue\n",
    "            \n",
    "# #             print('model_version', model_version)\n",
    "            \n",
    "# #             print(key, model_version)\n",
    "# #             print(f'ih5, ik, model_version, key, h5_coef_fn - {ih5}, {ik}, {model_version}, {key}, {h5_coef_fn}')\n",
    "\n",
    "#             y_col = key.split('_')[0].replace(r'/', r'')\n",
    "            \n",
    "#             h5_df = pd.read_hdf(h5f, key)\n",
    "\n",
    "#             h5_df.columns = [dfh.shorten_col_name(_) for _ in h5_df.columns]\n",
    "\n",
    "#             if 'dfrel_basis' != key.replace('/', ''):\n",
    "#                 h5_df.columns = [_.split('_')[0] for _ in h5_df.columns]\n",
    "#                 h5_df['true'] = h5_lst[ih5]['dfrel'][y_col]\n",
    "\n",
    "#                 h5_df['file_num'] = h5_lst[ih5]['dfrel']['file_num']\n",
    "\n",
    "#                 h5_df['dupe'] = h5_lst[ih5]['dfrel']['dupe']\n",
    "#                 h5_df['wi_trial_keep'] = h5_lst[ih5]['dfrel']['wi_trial_keep']\n",
    "\n",
    "#                 h5_lst[ih5][key] = h5_df\n",
    "#             else:\n",
    "#                 h5_lst[ih5]['dfrel'] = h5_df\n",
    "\n",
    "# #     for k in h5_lst:\n",
    "# #         for kk in h5_lst[k]:\n",
    "# #             print(k, kk)\n",
    "\n",
    "\n",
    "\n",
    "#     for ih5 in h5_lst:\n",
    "# #     for y_col in ['gACH', 'rDA', 'gDA']:\n",
    "# #             fig,axes = plt.subplots(5,2,figsize=(10,30))\n",
    "\n",
    "#         for key in h5_lst[ih5]:\n",
    "# #         for y_col in ['gACH', 'rDA', 'gDA']:\n",
    "# #         for ih5 in h5_lst:\n",
    "#             if key == 'dfrel':\n",
    "#                 continue\n",
    "            \n",
    "#             mse_ac = {}\n",
    "#             combo_rdc = {}\n",
    "#             for iac, alignment_col in enumerate(all_alignment_cols):\n",
    "#     #                 ic, ir = iac%2, iac//2\n",
    "#     #                 ax = axes[ir, ic]\n",
    "\n",
    "                \n",
    "#                 resp = key.split('_')[0].replace('/', '')\n",
    "#                 model_version = '_'.join(key.split('_')[2:]).split('_run_num')[0]\n",
    "#                 model_version = model_version.replace('5_base_words_gDA_to_gACH', '5_base_words_rDA_to_gACH')\n",
    "#                 print(key, alignment_col)\n",
    "#                 print(model_version)\n",
    "# #                 break\n",
    "# #             break\n",
    "# #         break\n",
    "# #     break\n",
    "#                 y_col = key.split('_')[0].replace(r'/', r'')\n",
    "                \n",
    "#                 dupe = h5_lst[ih5]['dfrel']['dupe']\n",
    "#                 dfrel_basis = h5_lst[ih5]['dfrel'].loc[~dupe]\n",
    "#                 dfrel_basis_run = h5_lst[ih5][key].loc[~dupe]\n",
    "\n",
    "#                 print('Pulling delta list')\n",
    "#                 keep_rows, assoc_deltas = get_delta_list(dfrel_basis[alignment_col], alignment_col, ((dfrel_basis_run['holdout']==1)).astype(bool))\n",
    "#                 rdc_lst = defaultdict(list)\n",
    "#                 for entry_num in trange(len(keep_rows)):\n",
    "#                     keep_row_single = keep_rows[entry_num]\n",
    "#                     deltas = assoc_deltas[entry_num]\n",
    "#                     rdc_df = dfrel_basis_run.loc[keep_row_single].set_index(deltas)\n",
    "\n",
    "#                     if rdc_df['file_num'].nunique() > 1:\n",
    "#                         continue\n",
    "\n",
    "#                     reconstruction_data_list = []\n",
    "\n",
    "#                     for bootstrap_col in ['true', 'pred']:\n",
    "#                         rdc_lst[bootstrap_col].append(rdc_df[[bootstrap_col]])\n",
    "\n",
    "#                 print('keys', rdc_lst.keys())\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 t, p = pd.concat(rdc_lst['true'], axis=1).sort_index(), pd.concat(rdc_lst['pred'], axis=1).sort_index()\n",
    "#                 dffsq = np.square(t.values - p.values)\n",
    "#                 mse_ac[alignment_col] = np.sum(dffsq[np.where(~np.isnan(dffsq))])/np.sum(~np.isnan(dffsq))\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "#                 seed = np.random.randint(1000000000)\n",
    "#                 for bootstrap_col in (rdc_lst):\n",
    "#                     tmp_rdc = pd.concat(rdc_lst[bootstrap_col], axis=1).sort_index()\n",
    "\n",
    "#                     reconstruction_data_list = []\n",
    "#                     for delta in (tmp_rdc.index):\n",
    "#                         row = tmp_rdc.loc[delta].dropna().values\n",
    "#                         bs = scipy.stats.bootstrap((row,), np.mean, n_resamples=9999,\n",
    "#     #                     bs = scipy.stats.bootstrap((row,), np.mean, n_resamples=1000,\n",
    "#                                                    batch=None, vectorized=True, paired=False,\n",
    "#                                                    axis=0, confidence_level=0.95, method='BCa',\n",
    "#                                                    random_state=seed)\n",
    "\n",
    "#                         mn_val = row.mean()\n",
    "#                         rtup = (bootstrap_col, delta, bs.confidence_interval.low, mn_val, bs.confidence_interval.high)\n",
    "\n",
    "#                         reconstruction_data_list.append(rtup)\n",
    "\n",
    "#                     tmp_combo_rdc = pd.DataFrame(reconstruction_data_list, columns=['basis_column', 'delta', 'lb', 'mn', 'ub']).set_index('delta')\n",
    "#                     combo_rdc[f'{alignment_col}_{bootstrap_col}'] = tmp_combo_rdc.drop('basis_column',axis=1)\n",
    "            \n",
    "#             combo_rdc = pd.concat(combo_rdc, axis=1)\n",
    "#             combo_rdc.columns = ['_'.join(_).replace('_true_', '_T_').replace('_pred_', '_P_') for _ in combo_rdc.columns]\n",
    "#             combo_rdc.to_csv(str((Path(out_loc) / f'{load_folder.parts[-1]}-reconstruct-{y_col}={model_version}.csv').resolve()))\n",
    "            \n",
    "#             import json\n",
    "#             with open(str((Path(out_loc) / f'{load_folder.parts[-1]}-mse_seg-{y_col}={model_version}.json').resolve()), 'w') as f:\n",
    "#                 json.dump(mse_ac, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [8]\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_rdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in h5_lst:\n",
    "#     for kk in h5_lst[k]:\n",
    "#         print(k, kk)\n",
    "# #         display(h5_lst[k][kk])\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(h5_lst['dfrel'][_]) for _ in range(len(h5_lst['dfrel']))], [len(h5_lst[y_col][_]) for _ in range(len(h5_lst[y_col]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dupe), len(h5_lst[y_col][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_rdc[[_ for _ in combo_rdc.columns if '_mn' in _]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col, model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_id_dct[y_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # base_location = Path(r'/Users/josh/Documents/Harvard/GLM/sabatinilab-glm/sglm/outputs')\n",
    "# base_location = Path(r'C:\\Users\\Josh\\Documents\\GitHub\\sabatinilab-glm\\sglm\\outputs')\n",
    "\n",
    "# fig_folder_lst = [#('Figure_1_2', 'f1*-ft'),\n",
    "#                   #('Figure_3',   'f3*-ft'),\n",
    "#                   #('Figure_4_g1',   'f4*-ft'),\n",
    "#                   ('Figure_4_g2',   'f4*-ft'),\n",
    "#                   #('Figure_5_g1',   'f5*-ft'),\n",
    "#                   #('Figure_5_g2',   'f5*-ft'),\n",
    "#                   #('Figure_5_g5',   'f5*-ft'),\n",
    "#                  ]\n",
    "\n",
    "# for fig_folder in fig_folder_lst:\n",
    "#     print('fig_folder', fig_folder)\n",
    "    \n",
    "#     load_folder = base_location / Path(fig_folder[0])\n",
    "#     # load_folder = base_location / Path(r'tmp')\n",
    "#     h5_recons_locations = str((load_folder / Path('all') / Path(fig_folder[1]) / Path(r'reconstructs\\best_resids_*.h5')).resolve())\n",
    "    \n",
    "#     print(h5_recons_locations)\n",
    "    \n",
    "# #     load_folder = base_location / Path(r'Figure_1_2')\n",
    "# #     # load_folder = base_location / Path(r'tmp')\n",
    "# #     h5_recons_locations = str((load_folder / Path(r'all\\f1*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "# #     # h5_recons_locations = str((load_folder / Path(r'all/f1*-ft/reconstructs/best_resids_*.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_3')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f3*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_4_g1')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f4*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_4_g2')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f4*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_5_g1')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f5*-ft\\reconstructs\\best_resids_*.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_5_g2')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f5*-ft\\coefs\\*_best_coeffs.h5')).resolve())\n",
    "# #     # load_folder = base_location / Path(r'Figure_5_g5')\n",
    "# #     # h5_beta_locations = str((load_folder / Path(r'all\\f5*-ft\\coefs\\*_best_coeffs.h5')).resolve())\n",
    "\n",
    "#     out_folder = base_location / Path(r'final_outputs')\n",
    "\n",
    "#     out_loc = str((out_folder).resolve())\n",
    "#     h5_recons_locations = glob.glob(h5_recons_locations)\n",
    "\n",
    "\n",
    "#     h5_lst = defaultdict(list)\n",
    "#     run_id_dct = defaultdict(list)\n",
    "\n",
    "\n",
    "#     for h5_coef_fn in h5_recons_locations:\n",
    "#         h5f = pd.HDFStore(h5_coef_fn)\n",
    "#         h5fk = h5f.keys()\n",
    "#         for key in h5fk:\n",
    "\n",
    "#             resp = key.split('_')[0].replace('/', '')\n",
    "#             model_version = '_'.join(key.split('_')[3:]).split('_run_num')[0]\n",
    "\n",
    "#             if 'run_num' in key and 'run_num__0' not in key: #and 'run_num__1' not in key:\n",
    "#                 continue\n",
    "#             if 'base_simple' not in key and 'base_words' not in key and 'basis' not in key:\n",
    "#                 continue\n",
    "#             print(key, model_version)\n",
    "\n",
    "#             y_col = key.split('_')[0].replace(r'/', r'')\n",
    "#             h5_df = pd.read_hdf(h5f, key)\n",
    "\n",
    "#             h5_df.columns = [dfh.shorten_col_name(_) for _ in h5_df.columns]\n",
    "\n",
    "#             if 'dfrel_basis' != key.replace('/', ''):\n",
    "#                 h5_df.columns = [_.split('_')[0] for _ in h5_df.columns]\n",
    "#                 h5_df['true'] = h5_lst['dfrel'][-1][y_col]\n",
    "#                 h5_df['resid2'] = (h5_df['true'] - h5_df['pred'])**2\n",
    "\n",
    "#                 h5_df['file_num'] = h5_lst['dfrel'][-1]['file_num']\n",
    "\n",
    "#                 h5_df['dupe'] = h5_lst['dfrel'][-1]['dupe']\n",
    "#                 h5_df['wi_trial_keep'] = h5_lst['dfrel'][-1]['wi_trial_keep']\n",
    "\n",
    "#                 h5_lst[y_col].append(h5_df)\n",
    "#                 run_id_dct[y_col].append(model_version)\n",
    "#             else:\n",
    "#                 h5_lst[y_col].append(h5_df)\n",
    "#                 run_id_dct[y_col].append('basis')\n",
    "#         break\n",
    "#     break\n",
    "\n",
    "#     for i in range(2):\n",
    "#         for y_col in ['gACH', 'rDA', 'gDA']:\n",
    "# #             fig,axes = plt.subplots(5,2,figsize=(10,30))\n",
    "\n",
    "#             combo_rdc = {}\n",
    "#             for iac, alignment_col in enumerate(all_alignment_cols):\n",
    "# #                 ic, ir = iac%2, iac//2\n",
    "# #                 ax = axes[ir, ic]\n",
    "\n",
    "#                 dupe = h5_lst['dfrel'][-1]['dupe']\n",
    "#                 dfrel_basis = h5_lst['dfrel'][-1].loc[~dupe]\n",
    "#                 dfrel_basis_run = h5_lst[y_col][i].loc[~dupe]\n",
    "\n",
    "#                 model_version = run_id_dct[y_col][i]\n",
    "\n",
    "#                 print('Pulling delta list')\n",
    "#                 keep_rows, assoc_deltas = get_delta_list(dfrel_basis[alignment_col], alignment_col, ((dfrel_basis_run['holdout']==1)).astype(bool))\n",
    "#                 rdc_lst = defaultdict(list)\n",
    "#                 for entry_num in trange(len(keep_rows)):\n",
    "#                     keep_row_single = keep_rows[entry_num]\n",
    "#                     deltas = assoc_deltas[entry_num]\n",
    "#                     rdc_df = dfrel_basis_run.loc[keep_row_single].set_index(deltas)\n",
    "\n",
    "#                     if rdc_df['file_num'].nunique() > 1:\n",
    "#                         continue\n",
    "\n",
    "#                     reconstruction_data_list = []\n",
    "\n",
    "#                     for bootstrap_col in ['true', 'pred']:\n",
    "#                         rdc_lst[bootstrap_col].append(rdc_df[[bootstrap_col]])\n",
    "\n",
    "#                 print('keys', rdc_lst.keys())\n",
    "\n",
    "#                 seed = np.random.randint(1000000000)\n",
    "#                 for bootstrap_col in (rdc_lst):\n",
    "#                     tmp_rdc = pd.concat(rdc_lst[bootstrap_col], axis=1).sort_index()\n",
    "\n",
    "#                     reconstruction_data_list = []\n",
    "#                     for delta in (tmp_rdc.index):\n",
    "#                         row = tmp_rdc.loc[delta].dropna().values\n",
    "#                         bs = scipy.stats.bootstrap((row,), np.mean, n_resamples=9999,\n",
    "#     #                     bs = scipy.stats.bootstrap((row,), np.mean, n_resamples=1000,\n",
    "#                                                    batch=None, vectorized=True, paired=False,\n",
    "#                                                    axis=0, confidence_level=0.95, method='BCa',\n",
    "#                                                    random_state=seed)\n",
    "\n",
    "#                         mn_val = row.mean()\n",
    "#                         rtup = (bootstrap_col, delta, bs.confidence_interval.low, mn_val, bs.confidence_interval.high)\n",
    "\n",
    "#                         reconstruction_data_list.append(rtup)\n",
    "\n",
    "#                     tmp_combo_rdc = pd.DataFrame(reconstruction_data_list, columns=['basis_column', 'delta', 'lb', 'mn', 'ub']).set_index('delta')\n",
    "#                     combo_rdc[f'{alignment_col}_{bootstrap_col}'] = tmp_combo_rdc.drop('basis_column',axis=1)\n",
    "\n",
    "#             combo_rdc = pd.concat(combo_rdc, axis=1)\n",
    "#             combo_rdc.columns = ['_'.join(_).replace('_true_', '_T_').replace('_pred_', '_P_') for _ in combo_rdc.columns]\n",
    "#             combo_rdc.to_csv(str((Path(out_loc) / f'{load_folder.parts[-1]}-reconstruct-{y_col}={model_version}.csv').resolve()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.all(h5_lst['dfrel'][0].dropna() == h5_lst['dfrel'][1].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5_lst['dfrel'][0]['rDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_lst.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa0fc083a9a7b25dab36cbe71fb89b2f1907d4eced1698b208dea6977346b521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
